<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>三点水</title>
  
  <subtitle>假装自己是人类</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://lotabout.github.io/"/>
  <updated>2018-07-29T02:13:50.117Z</updated>
  <id>http://lotabout.github.io/</id>
  
  <author>
    <name>Mark Wallace</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>QQA: Hibernate 为什么需要手工管理双向关联</title>
    <link href="http://lotabout.github.io/2018/QQA-Why-should-bidirectional-associations-be-managed-manually/"/>
    <id>http://lotabout.github.io/2018/QQA-Why-should-bidirectional-associations-be-managed-manually/</id>
    <published>2018-07-28T23:30:18.000Z</published>
    <updated>2018-07-29T02:13:50.117Z</updated>
    
    <content type="html"><![CDATA[<p>Hibernate/JPA 中如果两个 Entity 之间的关联是双向的（不论是 <code>@ManyToMany</code>、<code>@OneToMany</code> 还是 <code>@OneToOne</code>），都需要手动管理关联，为什么？</p><ul><li>调用 <code>entityManager.persist</code> 保存对象时 Hibernate/JPA 不会直接执行 SQL，而会等到 <code>entityManager.flush</code> 或事务 <code>commit</code> 时完成。</li><li>同理 <code>entityManager.load</code> 也可能只会从内存中获取对象(可以认为是某种缓存)。</li><li>如果不手动管理双向关联，则从内存获取的对象并不会反映数据库中的映射关系。</li></ul><h2 id="什么是双向关联"><a class="header-anchor" href="#什么是双向关联">#</a>什么是双向关联</h2><p>双向关联的本质是告诉 Hibernate 让两个实体共用一张数据库表(或表结构)。</p><p>这里以 <code>@ManyToMany</code> 为例(参考<a href="http://docs.jboss.org/hibernate/orm/5.3/userguide/html_single/Hibernate_User_Guide.html#associations-many-to-many-bidirectional" target="_blank" rel="noopener">Hibernate UserGuide</a>)：有两个实体 <code>Person</code> 和 <code>Address</code>，一个 Person 可以拥有多个 Address，而一个Address 也可以属于多个 Person。于是设计实体如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Entity</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Id</span></span><br><span class="line">    <span class="meta">@GeneratedValue</span></span><br><span class="line">    <span class="keyword">private</span> Long id;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@ManyToMany</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;Address&gt; addresses = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... omit all other stuff</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Entity</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Address</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Id</span></span><br><span class="line">    <span class="meta">@GeneratedValue</span></span><br><span class="line">    <span class="keyword">private</span> Long id;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@ManyToMany</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;Person&gt; owners = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... omit all other stuff</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>问题来了，我们应该创建一张关联表还是两张呢？其实取决于使用业务含义。即如果<code>Person</code> 中 <code>addresses</code> 的含义是“人的居住地址”，而 <code>Address</code> 中的 <code>owners</code> 与之对应，表达的是“地址上居住的人”，则它们应该是一张关联表。但如果 <code>Address</code> 的<code>owners</code> 表达的是“地址的主人(如房东)”，则二者就不应该共用一张关联表。</p><p>如何告诉 Hibernate 需要共用一张表呢？通过 <code>mappedBy</code>：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Entity</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="meta">@ManyToMany</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;Address&gt; addresses = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="comment">// ... omit all other methods</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Entity</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Address</span> </span>&#123;</span><br><span class="line">    <span class="meta">@ManyToMany</span>(mappedBy = <span class="string">"addresses"</span>)</span><br><span class="line">    <span class="keyword">private</span> List&lt;Person&gt; owners = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... omit all other methods</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>(mappedBy = &quot;addresses&quot;)</code> 的含义是这个字段与 <code>Person</code> 中的<code>addresses</code> 字段共用表结构。</p><p>这里最后重点是双向关系一定是从属关系，有一方是 owner，另一方是 follower(标记了<code>mappedBy</code> 的一方)。只有在 owner 这方添加关联并保存时，Hibernate 才会存入关联表，反之不会。例如我们只能通过 <code>person.addAddress()</code> 并保存 <code>person</code> 的方式来完成添加关联而不能用 <code>address.addPerson()</code> 后保存 <code>address</code> 的方式。</p><h2 id="手工管理关联是什么意思"><a class="header-anchor" href="#手工管理关联是什么意思">#</a>手工管理关联是什么意思</h2><p>例如我们在实现 <code>Person.addAddress</code> 时，需要这样实现：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Entity</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="comment">//...omit other fields</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@ManyToMany</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;Address&gt; addresses = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addAddress</span><span class="params">(Address address)</span> </span>&#123;</span><br><span class="line">        addresses.add( address );</span><br><span class="line">        address.getOwners().add( <span class="keyword">this</span> );</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">removeAddress</span><span class="params">(Address address)</span> </span>&#123;</span><br><span class="line">        addresses.remove( address );</span><br><span class="line">        address.getOwners().remove( <span class="keyword">this</span> );</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// ... omit all other methods</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>即在为 <code>person</code> 添加 <code>address</code> 时，我们需要将当前的 person 添加到 address的<code>owners</code> 字段中；删除时相似。“管理关联”表示需要在代码级别来管理关联双方实体的联系。</p><p>如果从数据库的角度思考，我们知道 <code>Person</code> 与 <code>Address</code> 的关系是存储在一张关联表里的，一个关联存入这张表后，不论哪一方读取，都应该反映出新的关联关系，而在Hibernate 这一层，却需要我们显式地(从另一方的 <code>set</code> )中添加/删除这个关联，显得不可思议。</p><p>另外，注意我们往 <code>set</code> 中添加 <code>address</code> 或 <code>person</code> 时，需要我们正确的实现<code>Person</code> 和 <code>Address</code> 的 <code>equals</code> 和 <code>hashCode</code> 方法，这是另一个坑，这里就不深入了。</p><h2 id="为什么需要手工管理"><a class="header-anchor" href="#为什么需要手工管理">#</a>为什么需要手工管理</h2><p>终于到了“为什么”部分了，首先是如果不手工管理会发生什么。考虑下面的测试：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="meta">@Transactional</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Person person = repository.findPersonById(<span class="number">1</span>);</span><br><span class="line">    Address address = repository.findAddressById(<span class="number">20</span>);</span><br><span class="line">    person.getAddresses.add(address);</span><br><span class="line">    repository.save(person);</span><br><span class="line"></span><br><span class="line">    System.out.println(address.getOwners().size()) <span class="comment">// what is the result?</span></span><br><span class="line"></span><br><span class="line">    Address address = repository.findAddressById(<span class="number">20</span>);</span><br><span class="line">    System.out.println(address.getOwners().size()) <span class="comment">// what is the result?</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>答案是两个 <code>size</code> 都为 <code>0</code>。</p><ul><li>调用 <code>save</code> 方法时，Hibernate/JPA 并不会直接执行 SQL 来保存，这样性能差。</li><li>在 <code>find</code> 时，如果内存中已经有对应的对象，Hibernate/JPA 也不会执行 SQL 去查询。</li></ul><p>注意上面说的是一般的情况，什么时候执行 SQL 取决于具体的配置，一般会在事务前的<code>commit</code>。</p><p>因此，如果在 <code>save</code> 之后还需要使用到 <code>address</code>，就不要期待它会立即反映出数据库中的修改；反之，如果 <code>save</code> 之后就不再使用到 <code>address</code>，那即使不手工管理(同步)关联关系也不会有多大影响。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hibernate/JPA 中如果两个 Entity 之间的关联是双向的（不论是 &lt;code&gt;@ManyToMany&lt;/code&gt;、
&lt;code&gt;@OneToMany&lt;/code&gt; 还是 &lt;code&gt;@OneToOne&lt;/code&gt;），都需要手动管理关联，为什么？&lt;/p&gt;

      
    
    </summary>
    
      <category term="QQA" scheme="http://lotabout.github.io/categories/QQA/"/>
    
    
      <category term="QQA" scheme="http://lotabout.github.io/tags/QQA/"/>
    
      <category term="hibernate" scheme="http://lotabout.github.io/tags/hibernate/"/>
    
      <category term="JPA" scheme="http://lotabout.github.io/tags/JPA/"/>
    
  </entry>
  
  <entry>
    <title>HTML 5 Drag and Drop 入门教程</title>
    <link href="http://lotabout.github.io/2018/HTML-5-Drag-and-Drop/"/>
    <id>http://lotabout.github.io/2018/HTML-5-Drag-and-Drop/</id>
    <published>2018-07-21T10:50:21.000Z</published>
    <updated>2018-07-29T02:13:50.089Z</updated>
    
    <content type="html"><![CDATA[<p>在 HTML 5 之前，想要实现 Drag and Drop（拖拽/拖放）一般需要求助于 JQuery，所幸HTML 5 已经把 DnD 标准化，现在我们能“轻易”地为几乎任意元素实现拖放功能。只是它的难度取决于你对 API 的理解程度，而<a href="https://developer.mozilla.org/en-US/docs/Web/API/HTML_Drag_and_Drop_API" target="_blank" rel="noopener">官方文档</a>并不好懂。这篇文章会一步步带你了解它的 API。</p><p>最终效果如下：</p><iframe scrolling="no" width="100%" height="300" src="//jsfiddle.net/lotabout/xhwsp1u6/13//embedded/result,js,html,css/light" frameborder="0" allowfullscreen></iframe><h2 id="拖动事件"><a class="header-anchor" href="#拖动事件">#</a>拖动事件</h2><p>继续之前，有必要先了解拖动时会触发哪些事件。考虑拖动 Source Element，途中经过Intermediate Element，最终进入 Target Element 并松开鼠标，则路径上会触发的事件如下图所示：</p><img src="/2018/HTML-5-Drag-and-Drop/drag-and-drop-events.svg" title="Drag and Drop Events"><p>这些事件的具体内容下面会讲到，你可以先跳过之后再回来查看，简单来说：</p><ol><li><code>dragstart</code>：当我们“拖”起元素时会触发。</li><li><code>dragenter</code>：当拖动元素 A 进入另一个元素 B 时，会触发 B 的 <code>dragenter</code> 事件。</li><li><code>dragleave</code>：与 <code>dragenter</code> 相对应，当拖动元素 A 离开元素 B 时，触发 B 的<code>dragleave</code> 事件。</li><li><code>dragover</code>：当拖动元素 A 在另一个元素 B 中移动/停止时触发 B 的 <code>dragover</code>事件。文档说是每几百毫秒触发一次，Chrome 实测 1ms 左右触发；Firefox 大概是300ms</li><li><code>drop</code>：当在拖动元素 A 到元素 B 上，释放鼠标时触发 B 的 <code>drop</code> 事件，相当于元素 B 接收了元素 A 。</li><li><code>dragend</code>：在 <code>drop</code> 事件之后，还会触发元素 A 的 <code>dragend</code> 事件，这里可以对元素 A 作一些清理工作。</li></ol><p>除了上面的事件外，还有两个一般用不到的事件：</p><ol><li><code>drag</code>：和 <code>dragover</code> 类似，当元素 A 被拖动时，每隔一段时间就会触发这个事件。与 <code>dragover</code> 不同，<code>drag</code> 事件是触发在源元素 A 上，而 <code>dragover</code> 是触发上潜在目标元素 B 上的。</li><li><code>dragexit</code>：这个事件只有 Firefox 支持，和 <code>dragleave</code> 作用几乎相同，发生在<code>dragleave</code> 之前。</li></ol><p>如果想实际验证一下这些事件是何时触发的，可以看看<a href="https://jsfiddle.net/lotabout/gq52cn3w/" target="_blank" rel="noopener">这个jsfiddle</a>，console 里会输出拖放的元素及对应的事件。下面我们开始一起实现咱们的拖放示例吧。</p><h2 id="让元素可拖放"><a class="header-anchor" href="#让元素可拖放">#</a>让元素可拖放</h2><p>一般在 HTML 里，元素默认是不可以作为源元素的（除了 <code>&lt;a&gt;</code>，<code>&lt;img&gt;</code>），例如一个<code>div</code>，我们是“拖不动”它的。这时只需要为它加上 <code>draggable=&quot;true&quot;</code> 属性它就能“拖”了。下面是我们的 DOM 结构：</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"drag-container"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"dropzone"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"draggable"</span> <span class="attr">draggable</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">      Drag Me</span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"dropzone"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"dropzone"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>draggable</code> 元素上加了 <code>draggable=&quot;true&quot;</code>，这样我们就能拖动它了，起码在 Chrome里可以，在 Firefox 里我们还需要在 <code>dragstart</code> 里为 <code>dataTransfer</code> 设置一些数据，因此需要加上下面的代码。具体的作用我们之后会说。</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">let</span> draggable = <span class="built_in">document</span>.getElementById(<span class="string">'draggable'</span>);</span><br><span class="line">draggable.addEventListener(<span class="string">'dragstart'</span>, (ev) =&gt; &#123;</span><br><span class="line">  ev.dataTransfer.setData(<span class="string">'text/plain'</span>, <span class="literal">null</span>);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>于是效果如下（CSS 没有贴出）：</p><iframe scrolling="no" width="100%" height="300" src="//jsfiddle.net/lotabout/xhwsp1u6/2//embedded/result,js,html,css/light" frameborder="0" allowfullscreen></iframe><p>这样红色的 <code>Drag Me</code> 元素就可以拖动了。下面我们增加一些拖动时的反馈，让交互更真实。</p><h2 id="添加拖动特效"><a class="header-anchor" href="#添加拖动特效">#</a>添加拖动特效</h2><p>首先，我们想在拖起元素让原始的元素变成半透明，这样当我们拖动时就会知道它是“真的可以拖动的”，而不是浏览器的什么奇怪行为。为此，我们可以监听 <code>dragstart</code> 事件：</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">draggable.addEventListener(<span class="string">"dragstart"</span>, (ev) =&gt; &#123;</span><br><span class="line"> ev.target.style.opacity = <span class="string">".5"</span>;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><iframe scrolling="no" width="100%" height="300" src="//jsfiddle.net/lotabout/xhwsp1u6/5//embedded/result,js,html,css/light" frameborder="0" allowfullscreen></iframe><p>这样一来我们开始拖动元素，它就变得透明了，然而我们松开鼠标，它依旧保持透明！这可不是我们想要的结果，因此我们需要监听 <code>dragend</code> 在拖动结束后还原透明度：</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">draggable.addEventListener(<span class="string">"dragend"</span>, (ev) =&gt; &#123;</span><br><span class="line">  ev.target.style.opacity = <span class="string">""</span>;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><iframe scrolling="no" width="100%" height="300" src="//jsfiddle.net/lotabout/xhwsp1u6/6//embedded/result,js,html,css/light" frameborder="0" allowfullscreen></iframe><p>下面，我们希望拖着元素 A 进入目标 B 时让 B 的边框变成虚线，以示意我们可以放入元素。</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">let</span> dropzones = <span class="built_in">document</span>.querySelectorAll(<span class="string">'.dropzone'</span>);</span><br><span class="line">dropzones.forEach(<span class="function">(<span class="params">dropzone</span>) =&gt;</span> &#123;</span><br><span class="line"></span><br><span class="line">  dropzone.addEventListener(<span class="string">'dragenter'</span>, (ev) =&gt; &#123;</span><br><span class="line">    ev.preventDefault();</span><br><span class="line">    dropzone.style.borderStyle = <span class="string">'dashed'</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">  dropzone.addEventListener(<span class="string">'dragover'</span>, (ev) =&gt; &#123;</span><br><span class="line">    ev.preventDefault();</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">  dropzone.addEventListener(<span class="string">'dragleave'</span>, (ev) =&gt; &#123;</span><br><span class="line">    dropzone.style.borderStyle = <span class="string">'solid'</span>;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>我们为所有的 <code>dropzone</code> 都监听了 <code>dragenter</code> 及 <code>dragleave</code> 事件，当拖动元素进入它们时，边框会变成虚线，离开时变回实线。这里有几个注意点：</p><ul><li>在 <code>dragenter</code> 与 <code>dragover</code> 里我们调用了 <code>ev.preventDefault()</code>，事实上几乎所有元素默认都是不允许 drop 发生的，这里调用<code>ev.preventDefault()</code> 可以阻止默认行为。</li><li>在 <code>dragenter</code> 中我们通过 <code>dropzone</code> 变量来修改样式而不是 <code>ev.target</code>，你可能觉得 <code>ev.target</code> 指向的是目标 B 元素，然而它指向的是源元素 A。</li><li>我们在 <code>dragenter</code> 而不是 <code>dragover</code> 中修改样式，是因为 <code>dragover</code> 会触发太频繁了。</li></ul><iframe scrolling="no" width="100%" height="300" src="//jsfiddle.net/lotabout/xhwsp1u6/12//embedded/result,js,html,css/light" frameborder="0" allowfullscreen></iframe><p>我们完成了“拖”的操作，最后需要完成“放”的操作了。</p><h2 id="数据传输-datatransfer"><a class="header-anchor" href="#数据传输-datatransfer">#</a>数据传输 DataTransfer</h2><p>拖动是最终目的是为了对源和目标元素做一些操作。为了完成操作，需要在源和目标传输数据，我们可以通过设置/读取全局变量来完成，这并不是一个好习惯。在 HTML 5 中，我们通过<a href="https://developer.mozilla.org/en-US/docs/Web/API/DataTransfer" target="_blank" rel="noopener">DataTransfer</a>完成。</p><p>我们在 <code>dragstart</code> 时设置需要传输的数据，在 drop 中获取需要的数据。<code>event.dataTransfer</code> 提供了两个主要函数：</p><ul><li><code>setData(format, data)</code>：用于添加数据，一般 format 对应于 MIME 类型字符串，常见的有 <code>text/plain</code>、<code>text/html</code> 及 <code>text/uri-list</code>等，但同时也可以是任意自定义的类型；不幸的是 data 只能是 <code>string</code> 或 <code>file</code>。</li><li><code>getData(format)</code>：用于获取数据。</li></ul><p>我们要实现将 <code>Drag Me</code> 放到其它蓝色元素中，需要传输它的 ID ，通过下面的代码实现：</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">draggable.addEventListener(<span class="string">'dragstart'</span>, (ev) =&gt; &#123;</span><br><span class="line">  ev.target.style.opacity = <span class="string">".5"</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 设置 ID</span></span><br><span class="line">  ev.dataTransfer.setData(<span class="string">'text/plain'</span>, ev.target.id);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">dropzones.forEach(<span class="function">(<span class="params">dropzone</span>) =&gt;</span> &#123;</span><br><span class="line">  dropzone.addEventListener(<span class="string">'drop'</span>, (ev) =&gt; &#123;</span><br><span class="line">    ev.preventDefault()</span><br><span class="line">    ev.target.style.borderStyle = <span class="string">'solid'</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取 ID</span></span><br><span class="line">    <span class="keyword">const</span> sourceId = ev.dataTransfer.getData(<span class="string">'text/plain'</span>)</span><br><span class="line">    ev.target.appendChild(<span class="built_in">document</span>.getElementById(sourceId))</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><ul><li>在 <code>dragstart</code> 时通过 <code>setData</code> 将 ID 放入 <code>DataTransfer</code> 中</li><li>在 <code>drop</code> 事件中，通过 <code>getData</code> 获取元素 ID 并通过 <code>appendChild</code> 加入到蓝色元素中。</li></ul><iframe scrolling="no" width="100%" height="300" src="//jsfiddle.net/lotabout/xhwsp1u6/13//embedded/result,js,html,css/light" frameborder="0" allowfullscreen></iframe><p>至此我们的简单示例就结束了，为了实现这么一个简单的示例，我们用到了全部的 6 个事件。因此从入门的角度来说 DnD API 并不容易，但换句话说这也就是它的几乎全部内容了，而你现在已经掌握了！恭喜！</p><h2 id="其它用法"><a class="header-anchor" href="#其它用法">#</a>其它用法</h2><p>定制拖放的行为时，还会有一些其它的需求，如拖放时的图标，到目标元素时鼠标的指针样式等，这里简单介绍一些。</p><p>当我们拖动元素时，浏览器默认生成了元素的缩略图，你可能需要自己设置，这时可以使用 <code>DataTransfer</code> 的 <code>setDragImage(image, xOffset, yOffset);</code> 函数。参考<a href="https://developer.mozilla.org/en-US/docs/Web/API/HTML_Drag_and_Drop_API/Drag_operations#dragfeedback" target="_blank" rel="noopener">MDN 上的例子</a>。</p><p><code>event.dataTransfer.dropEffect</code> 和 <code>event.effectAllowed</code> 共同决定了浏览器在执行拖动时的鼠标指针的行为，还有一些其它的用途。只是我实际测试时发现并不起作用，<a href="https://stackoverflow.com/questions/20471273/html5-drag-and-drop-effectallowed-and-dropeffect" target="_blank" rel="noopener">StackOverflow 的这个问题</a>说了一些自己的理解。</p><p>HTML5 还支持从操作系统中拖拽文件到浏览器中，或者从浏览器到操作系统中。如果从操作系统中获取文件，则可以访问 <code>event.dataTransfer.files</code> 字段，包含了操作系统中的文件内容。反之，在 <code>dragstart</code> 时正确设置 <code>event.dataTransfer.files</code> 则允许从浏览器中拖拽文件到操作系统中。</p><h2 id="一些坑"><a class="header-anchor" href="#一些坑">#</a>一些坑</h2><ul><li><code>dataTransfer</code> 的内容只在 <code>drop</code> 里可读，所以如果你想在 <code>dragEnter</code> 或<code>dragOver</code> 中通过 <code>dataTransfer.getData()</code> 返回的内容来决定一个目标元素是否允许放置是不可行的。其它的事件里只能通过一个个检查 <code>dataTransfer.items</code>里的 type 来获取已经设置的 <code>format</code> 而无法获取 <code>data</code>。</li><li><code>drop</code> 与 <code>dragend</code> 事件是顺序触发的，但在 <code>dragend</code> 里没有办法知道 <code>drop</code>事件是否已经触发。</li></ul><p>如果你遇到过其它的坑，也请在评论区留言～</p><h2 id="参考"><a class="header-anchor" href="#参考">#</a>参考</h2><ul><li><a href="https://www.html5rocks.com/en/tutorials/dnd/basics/" target="_blank" rel="noopener">Native HTML5 Drag and Drop</a> 经典的入门教程，一步步带你入门</li><li><a href="http://apress.jensimmons.com/v5/pro-html5-programming/ch9.html" target="_blank" rel="noopener">Working with HTML5 Drag-and-Drop</a> 相对更完整的介绍</li><li><a href="https://www.w3.org/TR/2011/WD-html5-20110113/dnd.html" target="_blank" rel="noopener">Drag and drop</a> W3CDnD 标准</li><li><a href="http://mereskin.github.io/dnd/" target="_blank" rel="noopener">HTML 5 drag and drop API</a> DnD 一些常见的坑</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在 HTML 5 之前，想要实现 Drag and Drop（拖拽/拖放）一般需要求助于 JQuery，所幸
HTML 5 已经把 DnD 标准化，现在我们能“轻易”地为几乎任意元素实现拖放功能。只是它的难度取决于你对 API 的理解程度，而&lt;a href=&quot;https:/
      
    
    </summary>
    
      <category term="Knowledge" scheme="http://lotabout.github.io/categories/Knowledge/"/>
    
    
      <category term="DnD" scheme="http://lotabout.github.io/tags/DnD/"/>
    
      <category term="HTML5" scheme="http://lotabout.github.io/tags/HTML5/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 入门介绍</title>
    <link href="http://lotabout.github.io/2018/kafka-introduction/"/>
    <id>http://lotabout.github.io/2018/kafka-introduction/</id>
    <published>2018-07-13T22:51:58.000Z</published>
    <updated>2018-07-29T02:13:50.125Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka 的大名相信大家早有耳闻，就是《变形记》的作者……咳咳……是一个著名的分布式消息队列，据说是因为作者特别喜欢作家卡夫卡才取名 Kafka 的。开始接触 Kafka 时最头疼的就是它的概念，什么是 group，什么是 partition …… 这里咱们从头开始理一理Kafka 的基本概念。</p><h2 id="topic"><a class="header-anchor" href="#topic">#</a>Topic</h2><p>一个 Topic（主题）对应一个消息队列。Kafka 支持多生产者，多消费者，对应下图：</p><img src="/2018/kafka-introduction/kafka-topic.svg" title="Kafka Topic"><p>多个生产者将数据发送到 Kafka 中，Kafka 将它们顺序存储，消费者的行为留到下面讨论。我们知道 Kafka 的目标是大数据，如果将消息存在一个“中心”队列中，势必缺少可伸缩性。无论是生产者/消费者数目的增加，还是消息数量的增加，都可能耗尽机器的性能或存储。</p><p>因此，Kafka 在概念上将一个 Topic 分成了多个 Partition，写入 topic 的消息会被（平均）分配到其中一个 Partition。Partition 中会为消息保存一个 Partition 内唯一的 ID ，一般称为偏移量(offset)。这样当性能/存储不足时 Kafka 就可以通过增加Partition 实现横向扩展。</p><img src="/2018/kafka-introduction/kafka-partition.svg" title="Kafka Partition"><p>现在我们有了一个队列的消息，那么如何发送给消费者呢？</p><h2 id="消费模型"><a class="header-anchor" href="#消费模型">#</a>消费模型</h2><p>一般有两种消费模型，不同模型下消费者的行为是不同的：</p><ul><li>队列模式（也叫点对点模式）。多个消费者共同消费一个队列，每条消息只发送给一个消费者。</li><li>发布/订阅模式。多个消费者订阅主题，每个消息会发布给所有的消费者。</li></ul><img src="/2018/kafka-introduction/kafka-consumer-model.svg" title="Kafka Consumer Model"><p>两种方式各有优缺点：</p><ul><li>队列模式中多个消费者共同消费同一个队列，效率高。</li><li>发布/订阅模式中，一个消息可以被多次消费，能支持冗余的消费（例如两个消费者共同消费一个消息，防止其中某个消费者挂了）</li></ul><p>显然要构建一个大数据下的消息队列，两种模式都是必须的。因此 Kafka 引入了Consumer Group（消费组）的概念，Consumer Group 是以发布/订阅模式工作的；一个 Consumer Group 中可以有多个 Consumer（消费者），Group 内的消费者以队列模式工作，如下图：</p><img src="/2018/kafka-introduction/kafka-consumer-group.svg" title="Kafka Consumer Group"><p>上面提到，Kafka 中的消息是以 Partition 存储的，那么它是如何与 Consumer 对接的呢？</p><h2 id="partition-与消费模型"><a class="header-anchor" href="#partition-与消费模型">#</a>Partition 与消费模型</h2><p>上面提到，Kafka 中一个 topic 中的消息是被打散分配在多个 Partition(分区) 中存储的，Consumer Group 在消费时需要从不同的 Partition 获取消息，那最终如何重建出 Topic中消息的顺序呢？</p><p>答案是：没有办法。Kafka 只会保证在 Partition 内消息是有序的，而不管全局的情况。</p><p>下一个问题是：Partition 中的消息可以被（不同的 Consumer Group）多次消费，那Partition中被消费的消息是何时删除的？ Partition 又是如何知道一个 ConsumerGroup 当前消费的位置呢？</p><ol><li>无论消息是否被消费，除非消息到期 Partition 从不删除消息。例如设置保留时间为2 天，则消息发布 2 天内任何 Group 都可以消费，2 天后，消息自动被删除。</li><li>Partition 会为每个 Consumer Group 保存一个偏移量，记录 Group 消费到的位置。如下图：</li></ol><img src="/2018/kafka-introduction/kafka-consumer-position.svg" title="Kafka Consumer Position"><p>上面我们提到的都是 Partition 与 Consumer Group 之间的关系，那 Group 中的Consumer 又是如何与 Partition 对应的呢？一般来说这也是最不容易理解的部分。但其实机制很简单：</p><ul><li>同一个 Consumer Group 内，一个 Partition 只能被一个 Consumer 消费。</li><li>推论1：如果 Consumer 的数量大于 Partition 数量，则会有 Consumer 是空闲的。</li><li>推论2：如果 Consumer 的数量小于 Partition 数量，则一个 Consumer 可能消费多个Partition。</li></ul><img src="/2018/kafka-introduction/kafka-partition-consumer.svg" title="Kafka Partition Consumer Relationship"><p>左边的 Consumer Group 中的 C4 是空闲的，而右边 Group 中的 C1 则需要消费两个Partition 。由于 C1 中消息可能来源于两个 Partition，此时如果需要确保消息的顺序，必须先判断消息的 Partition ID。</p><p>在分配 Partition 时，肯定是希望不同的 Consumer 的负载大致相同，具体的分配算法有 <code>Range</code> 的 <code>RoundRobin</code> 两种，文末会给出参考资料，这里不再赘述。</p><h2 id="物理存储"><a class="header-anchor" href="#物理存储">#</a>物理存储</h2><p>上面提到的 Topic, Partition 都是抽象的概念。每个 Partition 最终都需要存储在物理机器上，在 Kafka 中一般把这样的物理机器称为 <code>Broker</code>，可以是一台物理机，也可以是一个集群。</p><p>在讲概念的时候我们没有考虑到物理机可能会损坏的问题，这会导致某个 Partition 失效，上面存储的消息丢失，那还说什么高可用？所以一般需要对数据做冗余(replication)。换言之，需要存储多份 Partition 在不同的 Broker 上，并为它们的数据进行同步。那么从物理的视角：</p><img src="/2018/kafka-introduction/kafka-broker.svg" title="Kafka Partition Broker View"><p>上图中，某个 Topic 分成了 3 个 Partition，每个 Partition 保存了两个副本，副本平均分配到 3 个 Broker 上。图中即使有一个 Broker 挂了，剩余的两个 Broker 依旧能正常工作。这也是分布式系统的常用设计。</p><p>同一个 Partition 有多个副本，并分布在不同的 Broker 上，那么 Producer 应该写入到哪一个副本上呢？Consumer 又应该从哪个副本上读取呢？</p><ol><li>Kafka 的各个 Broker 需要与 Zookeeper 进行通信，每个 Partition 的多个副本之间通过 Zookeeper 的 Leader 选举机制选出主副本。所有该 Partition 上的读写都通过这个主副本进行。</li><li>其它的冗余副本会从主副本上同步新的消息。就像其它的 Consumer 一样。</li></ol><h2 id="小结"><a class="header-anchor" href="#小结">#</a>小结</h2><p>本文主要是对 Kafka 的基本概念和结构做了简要介绍，总结如下：</p><ol><li>Topic 是顶级概念，对应于一个消息队列。</li><li>Kafka 是以 Partition 为单位存储消息的，Consumer 在消费时也是按 Partition 进行的。即 Kafka 会保证一个 Consumer 收到的消息中，来自同一个 Partition 的所有消息是有序的。而来自不同 Partition 的消息则不保证有序。</li><li>Partition 会为其中的消息分配 Partition 内唯一的 ID，一般称作偏移量(offset)。Kafka 会保留所有的消息，直到消息的保留时间（例如设置保留 2 天）结束。这样Consumer 可以自由决定如何读取消息，例如读取更早的消息，重新消费等。</li><li>Kafka 有 Consumer Group 的概念。每个 Group 独立消费某个 Topic 的消息，互相不干扰。事实上，Kafka 会为每个 Group 保存一个偏移量，记录消费的位置。每个 Group 可以包含多个 Consumer，它们共同消费这个 Topic。</li><li>对于一个 Consumer Group，一个 Partition 只能由 Group 中的一个 Consumer 消费。具体哪个 Consumer 监听哪个 Partition 是由 Kafka 分配的。算法可以指定为<code>Range</code> 或 <code>RoundRobin</code>。</li><li>物理上，消息是存在 Broker 上的，一般对应为一台物理机或集群。存储时，每个Partition 都可以有多个副本。它们会被“均匀”地存储在各个 Broker 中。</li><li>对于一个 Partition，它的多个复本存储一般存储在不同 Broker 中，在同一时刻会由 Zookeeper 选出一个主副本来负责所有的读写操作。</li></ol><p>另外，随着 Kafka 的发展，它的定位已经从“分布式消息队列”变成了“分布式流处理平台”，添加了 Connector 及 Stream Processor 的概念。只是这些并不改变它的基本概念和结构。</p><h2 id="参考"><a class="header-anchor" href="#参考">#</a>参考</h2><ul><li><a href="http://kafka.apache.org/documentation/" target="_blank" rel="noopener">http://kafka.apache.org/documentation/</a> 官方文档，非常值得阅读</li><li><a href="http://www.ituring.com.cn/book/tupubarticle/18689" target="_blank" rel="noopener">http://www.ituring.com.cn/book/tupubarticle/18689</a> 更深入的 Kafka 技术细节</li><li><a href="https://sookocheff.com/post/kafka/kafka-in-a-nutshell/" target="_blank" rel="noopener">https://sookocheff.com/post/kafka/kafka-in-a-nutshell/</a> 图文并茂的 Kafka 教程</li><li><a href="https://www.iteblog.com/archives/2209.html" target="_blank" rel="noopener">https://www.iteblog.com/archives/2209.html</a> Kafka Partition 分配策略</li><li><a href="https://stackoverflow.com/questions/28574054/kafka-consumer-rebalancing-algorithm/28580363#28580363" target="_blank" rel="noopener">https://stackoverflow.com/questions/28574054/kafka-consumer-rebalancing-algorithm/28580363#28580363</a> SO 上对 Partition 分配策略的讲解</li><li><a href="https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/admin/AdminUtils.scala#L64" target="_blank" rel="noopener">https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/admin/AdminUtils.scala#L64</a> Broker 分配策略的源码，有详细解释</li><li><a href="https://blog.csdn.net/dly1580854879/article/details/71023553" target="_blank" rel="noopener">https://blog.csdn.net/dly1580854879/article/details/71023553</a> kafka Partition 分发策略的源码摘抄</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Kafka 的大名相信大家早有耳闻，就是《变形记》的作者……咳咳……是一个著名的分布式消息队列，据说是因为作者特别喜欢作家卡夫卡才取名 Kafka 的。开始接触 Kafka 时最头疼的就是它的概念，什么是 group，什么是 partition …… 这里咱们从头开始理一理
      
    
    </summary>
    
      <category term="Knowledge" scheme="http://lotabout.github.io/categories/Knowledge/"/>
    
    
      <category term="Kafka" scheme="http://lotabout.github.io/tags/Kafka/"/>
    
      <category term="Big Data" scheme="http://lotabout.github.io/tags/Big-Data/"/>
    
  </entry>
  
  <entry>
    <title>为 vim + tmux 开启真彩色(true color)</title>
    <link href="http://lotabout.github.io/2018/true-color-for-tmux-and-vim/"/>
    <id>http://lotabout.github.io/2018/true-color-for-tmux-and-vim/</id>
    <published>2018-07-11T23:02:11.000Z</published>
    <updated>2018-07-29T02:13:50.129Z</updated>
    
    <content type="html"><![CDATA[<p>有一些 vim 主题（如 <a href="https://github.com/morhetz/gruvbox" target="_blank" rel="noopener">gruvbox</a> 或<a href="https://github.com/lifepillar/vim-solarized8" target="_blank" rel="noopener">solarized8</a>）在 GUI 和终端下效果不同，有可能是因为这个主题需要 true color (24 位颜色) 的支持，而通常终端只开启 256 色的支持（如 <code>xterm-256color</code>）。下面来看看怎么开启 true color 支持。</p><h2 id="验证终端的色彩支持"><a class="header-anchor" href="#验证终端的色彩支持">#</a>验证终端的色彩支持</h2><p>真彩色的支持是需要终端的支持的，常用的终端（如 iterm2, konsole 等) 都已经支持了，详细的列表可以参考 <a href="https://gist.github.com/XVilka/8346728#now-supporting-truecolour" target="_blank" rel="noopener">Colours in terminal</a>。</p><p>当然，我们可以自己验证终端是否支持真彩色。在终端里执行<a href="https://github.com/gnachman/iTerm2/blob/master/tests/24-bit-color.sh" target="_blank" rel="noopener">24-bit-color.sh</a>脚本，如果支持真彩色，则显示如下：</p><img src="/2018/true-color-for-tmux-and-vim/true-color.png" title="True Color"><p>否则则类似下图：</p><img src="/2018/true-color-for-tmux-and-vim/256-color.png" title="Not True Color"><h2 id="tmux-开启真彩色"><a class="header-anchor" href="#tmux-开启真彩色">#</a>tmux 开启真彩色</h2><p>tmux &gt; 2.2 后开始支持真彩色。在 <code>.tmux.conf</code> 中添加如下内容：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set -g default-terminal screen-256color</span><br><span class="line">set-option -ga terminal-overrides &quot;,*256col*:Tc&quot; # 这句是关键</span><br></pre></td></tr></table></figure><p>重新开启 tmux 即可。注意要先退出所有正在运行的 tmux 后再重开 tmux。</p><h2 id="vim-开启真彩色"><a class="header-anchor" href="#vim-开启真彩色">#</a>vim 开启真彩色</h2><p><code>vim &gt;= 7.4.1770</code> 及 <code>neovim &gt;= 0.2.2</code> 都支持真彩色，但需要少许配置。在<code>.vimrc</code> 中加入：</p><figure class="highlight vim"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">has</span>(<span class="string">"termguicolors"</span>)</span><br><span class="line">    <span class="comment">" fix bug for vim</span></span><br><span class="line">    set t_8f=^[[38;2;%lu;%lu;%lum</span><br><span class="line">    set t_8b=^[[48;2;%lu;%lu;%lum</span><br><span class="line"></span><br><span class="line">    <span class="comment">" enable true color</span></span><br><span class="line">    <span class="keyword">set</span> termguicolors</span><br><span class="line"><span class="keyword">endif</span></span><br></pre></td></tr></table></figure><p>其中 <code>termguicolors</code> 用来开启真彩色，前面两行用来解决 vim 的 BUG (neovim 不需要），其中 <code>^[</code> 是代表 ESC 键，需要在 vim 中按 <code>Ctrl-v ESC</code> 来输入。</p><p>最后可以在 vim 中开启 terminal (vim 8 或 neovim 中执行 <code>:terminal</code>)，执行上面的 <code>24-bit-color.sh</code> 来验证是否成功。祝你的终端生活“丰富多彩”！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;有一些 vim 主题（如 &lt;a href=&quot;https://github.com/morhetz/gruvbox&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;gruvbox&lt;/a&gt; 或
&lt;a href=&quot;https://github.com/lifepi
      
    
    </summary>
    
      <category term="Notes" scheme="http://lotabout.github.io/categories/Notes/"/>
    
    
      <category term="vim" scheme="http://lotabout.github.io/tags/vim/"/>
    
      <category term="tmux" scheme="http://lotabout.github.io/tags/tmux/"/>
    
  </entry>
  
  <entry>
    <title>Reactor 模式简介</title>
    <link href="http://lotabout.github.io/2018/reactor-pattern/"/>
    <id>http://lotabout.github.io/2018/reactor-pattern/</id>
    <published>2018-06-28T10:07:24.000Z</published>
    <updated>2018-07-29T02:13:50.129Z</updated>
    
    <content type="html"><![CDATA[<p>如果你熟悉 Java 的 23 种设计模式，看到“Reactor 模式”可能就会一脸懵逼，这是什么鬼。Reactor 是一种应用在服务器端的开发模式（也有说法称 Reactor 是一种 IO 模式），目的是提高服务端程序的并发能力。</p><h2 id="reactor-模式"><a class="header-anchor" href="#reactor-模式">#</a>Reactor 模式</h2><p>它要解决什么问题呢？传统的 thread per connection 用法中，线程在真正处理请求之前首先需要从 socket 中读取网络请求，而在读取完成之前，线程本身被阻塞，不能做任何事，这就导致线程资源被占用，而线程资源本身是很珍贵的，尤其是在处理高并发请求时。</p><p>而 Reactor 模式指出，在等待 IO 时，线程可以先退出，这样就不会因为有线程在等待IO 而占用资源。但是这样原先的执行流程就没法还原了，因此，我们可以利用事件驱动的方式，要求线程在退出之前向 event loop 注册回调函数，这样 IO 完成时 eventloop 就可以调用回调函数完成剩余的操作。</p><p>所以说，Reactor 模式通过减少服务器的资源消耗，提高了并发的能力。当然，从实现角度上，事件驱动编程会更难写，难 debug 一些。</p><h2 id="餐厅里的-reactor-模式"><a class="header-anchor" href="#餐厅里的-reactor-模式">#</a>餐厅里的 Reactor 模式</h2><p>我们用“餐厅”类比的话，就像下图：</p><img src="/2018/reactor-pattern/reactor-old-servers.svg" title="Connection Per Thread"><p>对于每个新来的顾客，前台都需要找到一个服务员和厨师来服务这个顾客。</p><ol><li>服务员给出菜单，并等待点菜</li><li>顾客查看菜单，并点菜</li><li>服务员把菜单交给厨师，厨师照着做菜</li><li>厨师做好菜后端到餐桌上</li></ol><p>这就是传统的多线程服务器。每个顾客都有自己的服务团队（线程），在人少的情况下是可以良好的运作的。现在餐厅的口碑好，顾客人数不断增加，这时服务员就有点处理不过来了。</p><p>这时老板发现，每个服务员在服务完客人后，都要去休息一下，因此老板就说，“你们都别休息了，在旁边待命”。这样可能 10 个服务员也来得及服务 20 个顾客了。这也是“线程池”的方式，通过重用线程来减少线程的创建和销毁时间，从而提高性能。</p><p>但是客人又进一步增加了，仅仅靠剥削服务员的休息时间也没有办法服务这么多客人。老板仔细观察，发现其实服务员并不是一直在干活的，大部分时间他们只是站在餐桌旁边等客人点菜。</p><p>于是老板就对服务员说，客人点菜的时候你们就别傻站着了，先去服务其它客人，有客人点好的时候喊你们再过去。对应于下图：</p><img src="/2018/reactor-pattern/reactor-new-servers.svg" title="Server with Reactor"><p>最后，老板发现根本不需要那么多的服务员，于是裁了一波员，最终甚至可以只有一个服务员。</p><p>这就是 Reactor 模式的核心思想：减少等待。当遇到需要等待 IO 时，先释放资源，而在 IO 完成时，再通过事件驱动 (event driven) 的方式，继续接下来的处理。从整体上减少了资源的消耗。</p><h2 id="参考"><a class="header-anchor" href="#参考">#</a>参考</h2><ul><li><a href="https://www.cse.wustl.edu/~schmidt/PDF/reactor-siemens.pdf" target="_blank" rel="noopener">https://www.cse.wustl.edu/~schmidt/PDF/reactor-siemens.pdf</a> 原版论文，图文并茂</li><li><a href="https://blog.csdn.net/xxqi1229/article/details/39292661" target="_blank" rel="noopener">https://blog.csdn.net/xxqi1229/article/details/39292661</a> Reactor 模式的类比</li><li><a href="http://www.cnblogs.com/luxiaoxun/p/4331110.html" target="_blank" rel="noopener">Scalable IO in Java</a> 几种Reactor 模式的扩展</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;如果你熟悉 Java 的 23 种设计模式，看到“Reactor 模式”可能就会一脸懵逼，这是什么鬼。Reactor 是一种应用在服务器端的开发模式（也有说法称 Reactor 是一种 IO 模式），目的是提高服务端程序的并发能力。&lt;/p&gt;
&lt;h2 id=&quot;reactor-
      
    
    </summary>
    
      <category term="QQA" scheme="http://lotabout.github.io/categories/QQA/"/>
    
    
      <category term="reactor" scheme="http://lotabout.github.io/tags/reactor/"/>
    
      <category term="asyncio" scheme="http://lotabout.github.io/tags/asyncio/"/>
    
  </entry>
  
  <entry>
    <title>2^128 有多大</title>
    <link href="http://lotabout.github.io/2018/how-big-is-2-power-128/"/>
    <id>http://lotabout.github.io/2018/how-big-is-2-power-128/</id>
    <published>2018-06-27T20:55:55.000Z</published>
    <updated>2018-07-29T02:13:50.125Z</updated>
    
    <content type="html"><![CDATA[<p>编程中，我们常常需要为“数据”指定 ID，那什么样的类型才“够大”呢？<code>int</code>? <code>long</code>?<code>UUID</code>? 这篇文章里，咱们从直觉的角度聊一聊“数字”有多大。</p><p>首先为了简便计算，我们需要一个将 $2^a$ 转成 $10^b$ 的方法： $b \approx 0.3\times a$。如 $ 2^{10} \approx 10^3 $ 就是我们熟悉的 $ 1024 \approx 1000 $。证明的方法也就是简单地算算对数，这里不展开了。</p><h2 id="32-位"><a class="header-anchor" href="#32-位">#</a>32 位</h2><p>Java 中的 <code>int</code> 类型默认 32 位；C/C++ 中的类型具体长度与机器相关，但一般认为<code>int</code> 占 4 个字节，即 32 位；SQL 中的 <code>INT</code> 型也是 32 位。可以说 32 位是现代计算机系统中最常见的数据类型了。问题是，用它当 ID，“够大” 吗？</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2^32 = 4,294,967,296 (42亿) ~= 10^9</span><br></pre></td></tr></table></figure><p>那么用 <code>int</code> 来给全世界每人发一个 ID 肯定是不够用了，给中国每人发一个 ID 还是够用的，但考虑到每年的死亡和新增人口，恐怕不久 ID 也会用完。</p><p>另外地球的年龄是 45.43 亿年，因此用 <code>int</code> 也是没法满足的。</p><p>32 位还很容易换算成 <code>4G</code>，也因此 32 位计算机最多只能使用 4G 的内存（不完全正确，有一些其它的手段能超过 4G）。后来计算机推广到 64 位也与此有关。</p><p>尽管 32 位二进制看起来“不够大”，它也“不小”。假如记录心跳，假设一分钟 60 下，那么 32 位二进制可以用 <code>2^32/(60*60*24*365) = 136</code> 年。</p><p>因此 32 位适用于一些直觉上“不大”的数据。一般来说，如果数据不会随时间持续增加，就想想有没有中国人口多；随时间不断新增的数据，想想有没有心跳快。如果不会，都可以用 32 位来标识。</p><h2 id="64-位"><a class="header-anchor" href="#64-位">#</a>64 位</h2><p>Java 中的 <code>long</code>，C/C++ 中的 <code>long long</code>，SQL 中的 <code>BIGINT</code> 都指的是 64 位。那么它有多大呢？</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2^64 = 18,446,744,073,709,551,616 (1千亿亿) ~= 10^19</span><br></pre></td></tr></table></figure><p>这是多大呢？例如 2014 年双十一期间，支付宝的交易<a href="https://www.zhihu.com/question/26573474" target="_blank" rel="noopener">峰值是 285 万笔/分钟，</a> 假设用64位的 ID 来标记每一笔交易，可以用 <code>2^64/(2850000 * 60 * 24 * 365) = 12,314,577</code> 约一千万年。</p><p>另外，你知道宇宙中有多少星星吗？当然目前并没有权威的说明，但大概在 $10^{20}$ ~$10^{24}$ <a href="https://scienceline.ucsb.edu/getkey.php?key=3775" target="_blank" rel="noopener">左右</a>。显然用 64 位二进制并不足以标注宇宙中的每一颗星星，但它们基本是在一个量级上的。</p><p>因此这里的建议是，只要不是想数星星，用 64 位二进制来标识数据都是足够的。</p><h2 id="128-位"><a class="header-anchor" href="#128-位">#</a>128 位</h2><p>既然 64 位已经足够标识几乎所有的数据类型，128 位有用武之地吗？首先，64 位用来数星星还是不够的，其次是，我们还会有随机生成 ID 的需求，这也是 UUID/GUID 的用处。</p><p>新建数据时，我们常常需要为它们赋上 ID，且需要全局唯一。java 的 Hibernate 允许我们在数据生成时先暂时将 ID 置为 0，待存入数据库时再由数据库生成全局唯一的ID。但如果新生成的数据的 ID 需要被立即引用时（如生成一个图，每个节点都需要全局唯一ID，且新生成的节点会被其它节点引用），这个方法就行不通。当然可以专门提供一个ID生成器的服务来获取 ID，但它的复杂性就极大地上升了。</p><p>这时，使用 UUID 就是一个极佳的选择。它允许我们随机生成一个 UUID，且保证随机生成的 ID 是全局唯一的。当然，严格来说生成的 ID 还是会重复的，但概率极低，低到在实际使用时可以忽略。</p><p>参考<a href="https://zh.wikipedia.org/wiki/%E9%80%9A%E7%94%A8%E5%94%AF%E4%B8%80%E8%AF%86%E5%88%AB%E7%A0%81" target="_blank" rel="noopener">维基百科</a>，利用生日导论，可以计算两个随机生成的 UUID 重复的概率（真正有效的是 122 位，其它位被用来标识 UUID 的版本）是：</p><p>$$p(n) \approx 1-e^{- n^2/2x}$$</p><p>取 $x = 2^{122}$ 有下表：</p><table><thead><tr><th style="text-align:right">随机生成个数</th><th>重复概率</th></tr></thead><tbody><tr><td style="text-align:right">$68,719,476,736 = 2^{36}$</td><td>$0.0000000000000004 (4 \times 10^{-16})$</td></tr><tr><td style="text-align:right">$2,199,023,255,552 = 2^{41}$</td><td>$0.0000000000004 (4 \times 10^{-13})$</td></tr><tr><td style="text-align:right">$70,368,744,177,664 = 2^{46}$</td><td>$0.0000000004 (4 \times 10^{-10})$</td></tr></tbody></table><p>而如果要达到生成的 UUID 相同的概率超过 50% 则需要生成 $2.71 \times 10^{18}\approx 10^{60}$ 个 UUID。</p><p>因此，UUID 适用于需要随机生成全局唯一 ID 的情形。当然如果是密码学相关的数会需要更多的位数，这里就不讨论了。</p><p>顺代一提，IPv6 地址是 128 位的，号称能给整个地球的每一粒沙子都赋一个地址，其实128 位（$2^{128} \approx 10^{38}$）是远远超过<a href="https://www.zhihu.com/question/26825663" target="_blank" rel="noopener">地球的沙子数</a>（约 $10^{23}$）的，当然地址有一定规则，所以可用地址会少于 $2^{128}$ 个，这里也不讨论了。</p><h2 id="小结"><a class="header-anchor" href="#小结">#</a>小结</h2><p>如果数据量在直觉上不太，32 位的 ID 就足够；如果 32 位不够，则 64 位（几乎）一定够用；如果需要随机生成全局唯一 ID，使用 128 位的 UUID。</p><p>希望上面的讨论能指导大家选择 ID 的位数，同时增加一些选择时的信心。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;编程中，我们常常需要为“数据”指定 ID，那什么样的类型才“够大”呢？&lt;code&gt;int&lt;/code&gt;? &lt;code&gt;long&lt;/code&gt;?
&lt;code&gt;UUID&lt;/code&gt;? 这篇文章里，咱们从直觉的角度聊一聊“数字”有多大。&lt;/p&gt;
&lt;p&gt;首先为了简便计算，我们需要
      
    
    </summary>
    
      <category term="Notes" scheme="http://lotabout.github.io/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>分享创造 rargs</title>
    <link href="http://lotabout.github.io/2018/rargs/"/>
    <id>http://lotabout.github.io/2018/rargs/</id>
    <published>2018-04-14T09:50:33.000Z</published>
    <updated>2018-07-29T02:13:50.129Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/lotabout/rargs" target="_blank" rel="noopener">rargs</a> 是一个 rust 实现的命令行工具，它解决的是 <code>xargs</code> 或 <code>parallel</code> 等批量处理工具中无法自由引用输入的痛点。<code>rargs</code>支持用正则表达式来匹配输入中的任意内容。例如，我们想恢复一些以 <code>.bak</code> 结尾的备份文件，用 <code>rargs</code> 可以这么做：</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">ls *.bak | rargs -p <span class="string">'(.*)\.bak'</span> mv &#123;0&#125; &#123;1&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="批量重命名文件"><a class="header-anchor" href="#批量重命名文件">#</a>批量重命名文件</h2><p>我们先创建一些文件：</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ touch &#123;1..10&#125;.txt  <span class="comment"># &#123;&#125; 是 bash/zsh 的语法</span></span><br><span class="line">$ ls</span><br><span class="line">1.txt  2.txt  3.txt  4.txt  5.txt  6.txt  7.txt  8.txt  9.txt  10.txt</span><br></pre></td></tr></table></figure><p>现在我们把这些文件添上 <code>.bak</code> 后缀来备份。</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ ls | xargs -I&#123;&#125; mv &#123;&#125; &#123;&#125;.bak</span><br><span class="line">$ ls</span><br><span class="line">1.txt.bak  2.txt.bak  3.txt.bak  4.txt.bak  5.txt.bak  6.txt.bak  7.txt.bak  8.txt.bak  9.txt.bak  10.txt.bak</span><br></pre></td></tr></table></figure><p>可以看到 <code>xargs</code> 允许我们通过 <code>-I</code> 来指定占位符(placeholder)，代表输入行（具体化的使用方法麻烦查阅手册 <code>man xargs</code>），可以方便地实现批量处理。</p><p>那么如何批量地把这些文件还原呢？使用 <code>rargs</code> 就可以轻易地实现：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ls | rargs -p &apos;(.*).bak&apos; mv &#123;0&#125; &#123;1&#125;</span><br><span class="line">$ ls</span><br><span class="line">1.txt  2.txt  3.txt  4.txt  5.txt  6.txt  7.txt  8.txt  9.txt  10.txt</span><br></pre></td></tr></table></figure><p><code>rargs</code> 会用正则表达式 <code>(.*).bak</code> 匹配输入内容，然后会记录 <code>(...)</code> 中的内容（与正则表达式的语法一致），之后可以通过 <code>{1}</code> 来引用。</p><h2 id="批量下载"><a class="header-anchor" href="#批量下载">#</a>批量下载</h2><p>例如我们有一个 CSV 文件，存放着要下载文件的 URL 和文件名，存放格式如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">URL1,filename1</span><br><span class="line">URL2,filename2</span><br></pre></td></tr></table></figure><p>我们想用 <code>wget</code> 下载 URL 并保存成对应的文件名。用 <code>rargs</code> 可以这样实现：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat download-list.csv | rargs -p &apos;(?P&lt;url&gt;.*),(?P&lt;filename&gt;.*)&apos; wget &#123;url&#125; -O &#123;filename&#125;</span><br></pre></td></tr></table></figure><p>这里我们用了正则表达式中的 <code>(?P&lt;grou_name&gt;...)</code> 的语法，<code>rargs</code> 会保存group_name 和匹配到的内容，之后可以通过 <code>{group_name}</code> 引用 。</p><h2 id="替代-awk"><a class="header-anchor" href="#替代-awk">#</a>替代 AWK</h2><p>上面的例子用正则表达式来匹配 CSV 文件，如果字段多还是比较麻烦的。<code>rargs</code> 针对这种情况提供了 <code>-d ...</code> 来指定分隔符，之后可以像 AWK 一样通过 <code>{n}</code> 来引用第<code>n</code> 个字段。</p><p>不仅如此，我们经常会需要引用一些连续的字段，<code>rargs</code> 提供 <code>{start...end}</code> 的语法来引用 。</p><p>例如我们有一些 xSV 文件，如 <code>/etc/passwd</code>:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false</span><br><span class="line">root:*:0:0:System Administrator:/var/root:/bin/sh</span><br><span class="line">daemon:*:1:1:System Services:/var/root:/usr/bin/false</span><br></pre></td></tr></table></figure><p>我们可以用 <code>rargs</code> 来处理其中的字段：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cat /etc/passwd | rargs -d: echo -e &apos;id: &quot;&#123;1&#125;&quot;\t name: &quot;&#123;5&#125;&quot;\t rest: &quot;&#123;6..::&#125;&quot;&apos;</span><br><span class="line">id: &quot;nobody&quot;     name: &quot;Unprivileged User&quot;       rest: &quot;/var/empty:/usr/bin/false&quot;</span><br><span class="line">id: &quot;root&quot;       name: &quot;System Administrator&quot;    rest: &quot;/var/root:/bin/sh&quot;</span><br><span class="line">id: &quot;daemon&quot;     name: &quot;System Services&quot;         rest: &quot;/var/root:/usr/bin/false&quot;</span><br></pre></td></tr></table></figure><p><code>-d:</code> 指定了 <code>:</code> 为分隔符。<code>{6..}</code> 指定了第 6 及之后的字段。<code>{...:sep}</code> 语法可以指定 <code>sep</code>作为多个字段输出时使用的分隔符。</p><h2 id="多线程"><a class="header-anchor" href="#多线程">#</a>多线程</h2><p><code>rargs</code> 默认是顺序执行命令，如果需要多线程，可以通过</p><ul><li><code>-w &lt;num&gt;</code> 指定使用的线程数量</li><li><code>-w 0</code> 指定与 CPU 数量相同的线程数。</li></ul><h2 id="最后"><a class="header-anchor" href="#最后">#</a>最后</h2><p><code>rargs</code> 是一个简单的小工具，希望它能给你带来一些方便。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/lotabout/rargs&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;rargs&lt;/a&gt; 是一个 rust 实现的命令行工具，它解
决的是 &lt;code&gt;xargs&lt;/code&gt; 或 &lt;code&gt;parallel&lt;/code&gt; 等批量处理工具中无法自由引用输入的痛点。&lt;code&gt;rargs&lt;/code&gt;
支持用正则表达式来匹配输入中的任意内容。例如，我们想恢复一些以 &lt;code&gt;.bak&lt;/code&gt; 结尾的备
份文件，用 &lt;code&gt;rargs&lt;/code&gt; 可以这么做：&lt;/p&gt;
&lt;figure class=&quot;highlight sh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;ls *.bak | rargs -p &lt;span class=&quot;string&quot;&gt;&#39;(.*)\.bak&#39;&lt;/span&gt; mv &amp;#123;0&amp;#125; &amp;#123;1&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Project" scheme="http://lotabout.github.io/categories/Project/"/>
    
    
      <category term="rargs" scheme="http://lotabout.github.io/tags/rargs/"/>
    
      <category term="xargs" scheme="http://lotabout.github.io/tags/xargs/"/>
    
  </entry>
  
  <entry>
    <title>Python 元类 (MetaClass) 小教程</title>
    <link href="http://lotabout.github.io/2018/Understanding-Python-MetaClass/"/>
    <id>http://lotabout.github.io/2018/Understanding-Python-MetaClass/</id>
    <published>2018-04-06T15:43:59.000Z</published>
    <updated>2018-07-29T02:13:50.117Z</updated>
    
    <content type="html"><![CDATA[<p>可能是 Ruby 带的头，大家喜欢把“元编程”称作魔法，其实哪有什么魔法，一切都是科学。而 meta classes 就是 Python 里最魔法的科学，也是 99% 的人用不到的科学。只是谁还不想学点魔法呢？</p><p>（本文使用的语法仅在 Python 3 下有效）</p><h2 id="爷爷-元爸爸"><a class="header-anchor" href="#爷爷-元爸爸">#</a>爷爷 = 元爸爸</h2><blockquote><p>Meta is a prefix used in English to indicate a concept which is anabstraction behind another concept, used to complete or add to the latter.</p></blockquote><p>根据<a href="https://en.wikipedia.org/wiki/Meta" target="_blank" rel="noopener">维基百科</a>，英语前缀 <code>meta-</code> 指对一种抽象概念的抽象，得到另一种概念。比如说编程 programming 一般指编写代码来读取、生成或转换<strong>数据</strong>。那么元编程 <code>meta-programming</code> 一般就指人编写代码来读取、生成或转换<strong>代码</strong>。</p><p>听着很玄幻，但 meta 让我想到了一首儿歌，有句歌词：“爸爸的爸爸叫什么？爸爸的爸爸叫爷爷”，现在有了 <code>meta-</code>，我们可以把爷爷叫作 <code>meta-爸爸</code>（元爸爸）了。</p><p>我们知道 Python 里一切都是对象，那么是对象就有对应的“类(Class)”，或称“类型(type)”。Python 中可以用 <code>type(obj)</code> 来得到对象的“类”：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">type(<span class="number">10</span>)</span><br><span class="line"><span class="comment">#&gt; int</span></span><br><span class="line">type([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="comment">#&gt; list</span></span><br><span class="line">type(&#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;)</span><br><span class="line"><span class="comment">#&gt; dict</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoNothing</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">x = DoNothing()</span><br><span class="line">type(x)</span><br><span class="line"><span class="comment">#&gt; __main__.DoNothing</span></span><br></pre></td></tr></table></figure><p>既然一切都是对象，一个“类(class)”也可以认为是一个对象，那么类的“类型(type)”是什么呢？</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">type(int), type(list), type(dict)</span><br><span class="line"><span class="comment">#&gt; (type, type, type)</span></span><br><span class="line"></span><br><span class="line">type(DoNothing)</span><br><span class="line"><span class="comment">#&gt; type</span></span><br></pre></td></tr></table></figure><p>可以看到，“类(class)”的类型(type) 都是 <code>type</code>。那 <code>type</code> 的类型又是什么呢？</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">type(type)</span><br><span class="line"><span class="comment">#&gt; type</span></span><br></pre></td></tr></table></figure><p>抱歉，<code>type</code> 的类型还是 <code>type</code>，是一个递归的类型。</p><p>对象的类型叫作类(class)，<strong>类的类型就称作元类 <code>meta-class</code></strong>。是不是很像“爸爸的爸爸叫爷爷”？换句话说，“普通类(class)”可以用来生成实例(instance)，同样的，元类(meta-class)也可以生成实例，生成的实例就是“普通类”了。</p><h2 id="类是动态创建的"><a class="header-anchor" href="#类是动态创建的">#</a>类是动态创建的</h2><p>我们知道，类(class)可以有多个实例(instance)。而创建实例的方法就是调用类的构造函数(constructor)：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spam</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line"></span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">spam = Spam(<span class="string">'name'</span>)</span><br></pre></td></tr></table></figure><p>上例我们定义了一个类，并调用类的构造函数创建了该类的一个实例。我们知道类也可以看作类 <code>type</code> 的一个实例，那么如何用 <code>type</code> 的构造函数来动态创建一个类呢？我们先看看 <a href="https://docs.python.org/3.6/library/functions.html#type" target="_blank" rel="noopener">type 的构造函数</a>：</p><p>type(name, bases, dict):</p><ul><li><code>name</code>: 字符串类型，存放新类的名字</li><li><code>bases</code>: 元组(tuple)类型，指定类的基类/父类</li><li><code>dict</code>: 字典类型，存放该类的所有属性(attributes)和方法(method)</li></ul><p>例如下面的类：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span>:</span></span><br><span class="line">    counter = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_counter</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.counter</span><br><span class="line"></span><br><span class="line">x = Derived()</span><br><span class="line">x.get_counter()</span><br><span class="line"><span class="comment">#&gt; 10</span></span><br></pre></td></tr></table></figure><p>我们可以调用 <code>type(...)</code> 来动态创建这两个类：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Base = type(<span class="string">'Base'</span>, (), &#123;<span class="string">'counter'</span>: <span class="number">10</span>&#125;)</span><br><span class="line">Derived = type(<span class="string">'Derived'</span>, (Base,), dict(get_counter=<span class="keyword">lambda</span> self: self.counter))</span><br><span class="line"></span><br><span class="line">x = Derived()</span><br><span class="line">x.get_counter()</span><br><span class="line"><span class="comment">#&gt; 10</span></span><br></pre></td></tr></table></figure><p>是的，你没有猜错，Python 在遇到 <code>class ...</code> 关键字时会一步步解析类的内容，最终调用 <code>type(...)</code> （准确说是指定的元类）的构造函数来创建类，换句话说上面两种定义类的方式是等价的。在下节我们会具体讲解。</p><h2 id="类的创建过程"><a class="header-anchor" href="#类的创建过程">#</a>类的创建过程</h2><p>要了解元类(meta-class)的作用，我们就需要了解 Python 里<a href="https://docs.python.org/3/reference/datamodel.html#metaclasses" target="_blank" rel="noopener">类的创建过程</a>，如下：</p><img src="/2018/Understanding-Python-MetaClass/class-creation.svg" title="Class Creation Step in Python"><ol><li>当 Python 见到 <code>class</code> 关键字时，会首先解析 <code>class ...</code> 中的内容。例如解析基类信息，最重要的是找到对应的元类信息（默认是 <code>type</code>)。</li><li>元类找到后，Python 需要准备 namespace （也可以认为是上节中 <code>type</code> 的 <code>dict</code>参数）。如果元类实现了 <code>__prepare__</code> 函数，则会调用它来得到默认的 namespace。</li><li>之后是调用 <code>exec</code> 来执行类的 body，包括属性和方法的定义，最后这些定义会被保存进 namespace。</li><li>上述步骤结束后，就得到了创建类需要的所有信息，这时 Python 会调用元类的构造函数来真正创建类。</li></ol><p>如果你想在类的创建过程中做一些定制(customization)的话，创建过程中任何用到了元类的地方，我们都能通过覆盖元类的默认方法来实现定制。这也是元类“无所不能”的所在，它深深地嵌入了类的创建过程。</p><h2 id="元类的应用"><a class="header-anchor" href="#元类的应用">#</a>元类的应用</h2><blockquote><p>元类就是深度的魔法，99%的用户应该根本不必为此操心。如果你想搞清楚究竟是否需要用到元类，那么你就不需要它。那些实际用到元类的人都非常清楚地知道他们需要做什么，而且根本不需要解释为什么要用元类。</p><footer><strong>Python界的领袖 Tim Peters</strong></footer></blockquote><p>为了文章的完整性，以及日后查阅方便，这里还是要举两个例子的。顺带一提，下面这两个例子在 Python 3.6 之后都可以通过覆盖基类的<a href="https://docs.python.org/3/reference/datamodel.html#object.__init_subclass__" target="_blank" rel="noopener">__init_subclass__</a>来实现，而不需要通过元类实现。</p><h3 id="强制子类实现特定方法"><a class="header-anchor" href="#强制子类实现特定方法">#</a>强制子类实现特定方法</h3><p>假设你是一个库的作者，例如下面的代码，其中的方法 <code>foo</code> 要求子类实现方法 <code>bar</code>：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># library code</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.bar()</span><br><span class="line"></span><br><span class="line"><span class="comment"># user code</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br></pre></td></tr></table></figure><p>但作为库的作者，我们根本无法预测用户会写出什么样的代码，有什么方法能强制用户在子类中实现方法 <code>bar</code> 呢？用 meta-class 可以做到。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Meta</span><span class="params">(type)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(cls, name, bases, namespace, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> name != <span class="string">'Base'</span> <span class="keyword">and</span> <span class="string">'bar'</span> <span class="keyword">not</span> <span class="keyword">in</span> namespace:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">'bad user class'</span>)</span><br><span class="line">        <span class="keyword">return</span> super().__new__(cls, name, bases, namespace, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span><span class="params">(object, metaclass=Meta)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.bar()</span><br></pre></td></tr></table></figure><p>现在，我们尝试定义一个不包含 <code>bar</code> 方法的子类，在类的定义（或者说生成）阶段就会报错：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; class Derived(Base):</span><br><span class="line">...     pass</span><br><span class="line">...</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">  File &quot;&lt;stdin&gt;&quot;, line 4, in __new__</span><br><span class="line">TypeError: bad user class</span><br></pre></td></tr></table></figure><h3 id="注册所有子类"><a class="header-anchor" href="#注册所有子类">#</a>注册所有子类</h3><p>有时我们会希望获取继承了某个类的子类，例如，实现了基类 <code>Fruit</code>，想知道都有哪些子类继承了它，用元类就能实现这个功能：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Meta</span><span class="params">(type)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(cls, name, bases, namespace, **kwargs)</span>:</span></span><br><span class="line">        super().__init__(name, bases, namespace, **kwargs)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(cls, <span class="string">'registory'</span>):</span><br><span class="line">            <span class="comment"># this is the base class</span></span><br><span class="line">            cls.registory = &#123;&#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># this is the subclass</span></span><br><span class="line">            cls.registory[name.lower()] = cls</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fruit</span><span class="params">(object, metaclass=Meta)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Apple</span><span class="params">(Fruit)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Orange</span><span class="params">(Fruit)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>之后，我们可以查看所有 <code>Fruit</code> 的子类：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; Fruit.registory</span><br><span class="line">&#123;&apos;apple&apos;: &lt;class &apos;__main__.Apple&apos;&gt;, &apos;orange&apos;: &lt;class &apos;__main__.Orange&apos;&gt;&#125;</span><br></pre></td></tr></table></figure><h3 id="new-vs-init"><a class="header-anchor" href="#new-vs-init">#</a>new vs init</h3><p>上面的例子中我们分别用了 <code>__new__</code> 和 <code>__init__</code>，但其实这两个例子里用哪种方法都是可行的。</p><p><code>__new__</code> 用来创建一个（未初始化）实例；<code>__init__</code> 则是用来初始化一个实例。在元类的 <code>__new__</code> 方法中，因为类实例还没有创建，所以可以更改最后生成类的各项属性：诸如名称，基类或属性，方法等。而在 <code>__init__</code> 中由于类已经创建完成，所以无法改变。正常情况下不需要关心它们的区别。</p><h2 id="小结"><a class="header-anchor" href="#小结">#</a>小结</h2><ul><li>对象的类型称为类，类的类就称为元类。</li><li>Python 中对元类实例化的结果就是“普通类”，这个过程是动态的。</li><li>在定义类时可以指定元类来改变类的创建过程。</li></ul><p>请你相信，作为平民百姓，咱们是没有机会用到魔法的。但学习本身对于了解语言的设计是很有帮助的，何况万一有个万一呢◔_◔？</p><h2 id="参考"><a class="header-anchor" href="#参考">#</a>参考</h2><ul><li><a href="https://jakevdp.github.io/blog/2012/12/01/a-primer-on-python-metaclasses/" target="_blank" rel="noopener">A Primer on Python Metaclasses</a> 一步步教你理解 meta class</li><li><a href="https://blog.ionelmc.ro/2015/02/09/understanding-python-metaclasses/" target="_blank" rel="noopener">Understanding Python metaclasses</a> 对 Python 中的 attribute lookup 有清晰的讲解</li><li><a href="https://www.youtube.com/watch?v=sPiWg5jSoZI" target="_blank" rel="noopener">Python 3 Metaprogramming</a> 3 小时的视频，风趣幽默地对讲解了 Python 元编程的各个方面。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;可能是 Ruby 带的头，大家喜欢把“元编程”称作魔法，其实哪有什么魔法，一切都是科学。而 meta classes 就是 Python 里最魔法的科学，也是 99% 的人用不到的科学。只是谁还不想学点魔法呢？&lt;/p&gt;
&lt;p&gt;（本文使用的语法仅在 Python 3 下有效）
      
    
    </summary>
    
      <category term="Knowledge" scheme="http://lotabout.github.io/categories/Knowledge/"/>
    
    
      <category term="python" scheme="http://lotabout.github.io/tags/python/"/>
    
      <category term="meta-programming" scheme="http://lotabout.github.io/tags/meta-programming/"/>
    
  </entry>
  
  <entry>
    <title>QQA: Python 中的 str 与 repr</title>
    <link href="http://lotabout.github.io/2018/QQA-Python-str-vs-repr/"/>
    <id>http://lotabout.github.io/2018/QQA-Python-str-vs-repr/</id>
    <published>2018-04-04T17:09:37.000Z</published>
    <updated>2018-07-29T02:13:50.117Z</updated>
    
    <content type="html"><![CDATA[<p>有时候，你会需要为你的类实现 <code>__str__</code> 或 <code>__repr__</code> 方法，你知道它们的作用是什么吗？它们有什么区别吗？这个问题的答案一搜就能找到，如果恰巧这是你第一次看到这个问题，不妨看看吧。</p><a id="more"></a><ul><li><code>__repr__</code> 用于生成<strong>正式</strong>的表示。可以认为是将对象序列化的方法，原则上要能反序列化回对象。</li><li><code>__str__</code> 用于生成<strong>非正式</strong>的表示。<code>format</code> 或 <code>print</code> 会调用它来为用户生成“友好的”显示。</li><li>如果你需要自己实现，一般实现 <code>__str__</code> 即可。</li></ul><h2 id="python-中一切皆对象"><a class="header-anchor" href="#python-中一切皆对象">#</a>Python 中一切皆对象</h2><p><a href="https://docs.python.org/3/reference/datamodel.html" target="_blank" rel="noopener">Python Data Model</a> 中指出，Python 中的所有数据都是“对象”(Object)。Python 中几乎所有（不确定有没有反例）的操作都可以对应到对象的某个特殊方法。因此可以通过手工实现它们来覆盖默认的逻辑。</p><p>比如说迭代器(iterator)取长度操作 <code>len(iter)</code> 对应 <code>obj.__len__</code>；加法操作 <code>a + b</code>对应<code>a.__add__(b)</code>；函数调用 <code>func(...)</code> 对应 <code>func.__cal__(...)</code>。当然也包括我们要介绍的 <code>__repr__</code> 和 <code>__str__</code>。</p><h2 id="repr-用于-debug"><a class="header-anchor" href="#repr-用于-debug">#</a>repr 用于 Debug</h2><p>Python 中执行 <code>repr(obj)</code> 可以获取 <code>obj</code> 的字符串表示，而这个操作相当于调用了<a href="https://docs.python.org/3/reference/datamodel.html#object.__repr__" target="_blank" rel="noopener">obj.__repr__()</a>。而这个字符串表示“原则上”需要能反序列化回 <code>obj</code> 本身。看下面代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">repr(x)</span><br><span class="line"><span class="comment">#&gt; '[1, 2, 3]'</span></span><br><span class="line">eval(<span class="string">'[1, 2, 3]'</span>)</span><br><span class="line"><span class="comment">#&gt; [1, 2, 3]</span></span><br><span class="line">eval(repr(x)) == x</span><br><span class="line"><span class="comment">#&gt; True</span></span><br></pre></td></tr></table></figure><p>我们看到 <code>'[1, 2, 3]'</code> (注意到逗号后面带了空格) 是数据 <code>[1,2,3]</code> 的字符串表示，用 <code>eval</code> 来反序列化可以得到原数据。那么如果变量是自定义的类又如何呢？</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, arg)</span>:</span></span><br><span class="line">        self.arg = arg</span><br><span class="line"></span><br><span class="line">x = MyClass(<span class="number">10</span>)</span><br><span class="line">repr(x)</span><br><span class="line"><span class="comment">#&gt; '&lt;__main__.MyClass object at 0x10a40ef98&gt;'</span></span><br></pre></td></tr></table></figure><p>可以看到，<code>repr(x)</code> 给出了对象的类型及对象的 ID (内存地址)。但如果我们用<code>eval(repr(x))</code> 尝试反序列化时，会失败。所以能反序列化其实是一个约定而非强制。我们尝试覆盖默认的实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, arg)</span>:</span></span><br><span class="line">        self.arg = arg</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'MyClass(&#123;&#125;)'</span>.format(self.arg)</span><br><span class="line"></span><br><span class="line">x = MyClass(<span class="number">10</span>)</span><br><span class="line">repr(x)</span><br><span class="line"><span class="comment">#&gt; 'MyClass(10)'</span></span><br><span class="line">eval(repr(x))</span><br><span class="line"><span class="comment">#&gt; MyClass(10)</span></span><br></pre></td></tr></table></figure><p>可以看到对 <code>__repr__</code> 的覆盖起了效果，也可以正常反序列化了。</p><p>上面这几个例子都是为了说明 <code>repr</code> 生成的字符串到底有什么用。</p><ul><li><code>repr</code> 并<strong>不强制</strong>生成的字符串可以反序列化</li><li><code>repr</code> 生成的字符串一般用于 debug，所以一般生成的字符串一般要包含尽可能多的信息，信息要尽可能明确(如默认实现里用 ID 区分开两个不同的对象)。</li><li>不要使用 <code>repr</code> 和 <code>eval</code> 来做序列化/反序列化，用 <a href="https://docs.python.org/3/library/pickle.html" target="_blank" rel="noopener">pickle</a> 或 <a href="https://docs.python.org/3/library/pickle.html" target="_blank" rel="noopener">json</a>。</li></ul><h2 id="str-用于显示"><a class="header-anchor" href="#str-用于显示">#</a>str 用于显示</h2><p><a href="https://docs.python.org/3/reference/datamodel.html#object.__str__" target="_blank" rel="noopener">obj.__str__()</a>方法会在 <code>print(obj)</code> 或 <code>'{}'.format(obj)</code> 时被调用，一般是为了给用户提供&quot;友好的&quot;显示，所以 <code>__str__</code> 不像 <code>__repr__</code> 那样原则上对返回值有约定，想怎么搞都行。</p><p>另外，<code>__str__</code> 的默认实现是直接调用了 <code>__repr__</code> 方法。因此如果覆盖了<code>__repr__</code> 方法，<code>__str__</code> 的结果也会随之改变。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有时候，你会需要为你的类实现 &lt;code&gt;__str__&lt;/code&gt; 或 &lt;code&gt;__repr__&lt;/code&gt; 方法，你知道它们的作用
是什么吗？它们有什么区别吗？这个问题的答案一搜就能找到，如果恰巧这是你第一次看
到这个问题，不妨看看吧。&lt;/p&gt;
    
    </summary>
    
      <category term="QQA" scheme="http://lotabout.github.io/categories/QQA/"/>
    
    
      <category term="python" scheme="http://lotabout.github.io/tags/python/"/>
    
      <category term="QQA" scheme="http://lotabout.github.io/tags/QQA/"/>
    
      <category term="str" scheme="http://lotabout.github.io/tags/str/"/>
    
      <category term="repr" scheme="http://lotabout.github.io/tags/repr/"/>
    
  </entry>
  
  <entry>
    <title>QQA: 为什么 java 中要写 getter/setter？</title>
    <link href="http://lotabout.github.io/2018/QQA-why-use-getters-and-setters/"/>
    <id>http://lotabout.github.io/2018/QQA-why-use-getters-and-setters/</id>
    <published>2018-04-02T19:06:30.000Z</published>
    <updated>2018-07-29T02:13:50.117Z</updated>
    
    <content type="html"><![CDATA[<p>java 有一个不成文的规定，如果要访问一个类的 private 字段，就需要写getter/setter 方法。但我们在其它语言却很少见到类似的约定，为什么？</p><ul><li>它是“封装”的体现，对外隐藏了具体实现，允许之后对属性的访问注入新的逻辑（如验证逻辑）。</li><li>一些语言，如 python，提供了机制允许我们更改访问属性的逻辑，因此不需要手工写getter/setter。</li></ul><h2 id="getter-setter-是对-属性访问-的封装"><a class="header-anchor" href="#getter-setter-是对-属性访问-的封装">#</a>getter/setter 是对“属性访问”的封装</h2><p>假设我们写了下面这段代码，直接访问类的 public 字段：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// caller</span></span><br><span class="line">String name = person.name;</span><br><span class="line">person.name = <span class="string">"Java"</span>;</span><br></pre></td></tr></table></figure><p>之后我们认为 <code>name</code> 属性只能是字母，不能包含其它的字符，上面这种实现中，我们就需要更改所有 caller 调用 <code>person.name = ...</code> 的代码。换句话说，类 <code>Person</code> 暴露了实现的细节（即字段 person）。</p><p>那么如果一开始就使用了 getter/setter，则我们不需要改变任何 caller，只需要在<code>setName</code> 函数里增加相应的逻辑即可。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        validate_name(name);  <span class="comment">// the newly added validation logic</span></span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// caller</span></span><br><span class="line">String name = person.getName();</span><br><span class="line">person.setName(<span class="string">"Java"</span>);</span><br></pre></td></tr></table></figure><p>所以，通过这层封装，之后如果有需要，我们甚至可以更改字段的名字，类型等等。这就是封装的好处，而 getter/setter 这种写法能让我们为将来可能的修改做好准备。</p><h2 id="其它语言里的-getter-setter"><a class="header-anchor" href="#其它语言里的-getter-setter">#</a>其它语言里的 getter/setter</h2><p>getter/setter 的作用是为“属性的访问”（即 <code>x.field</code> 与 <code>x.field = ...</code>）提供日后修改的可能。一些“比较新”的语言就默认提供了这种能力。</p><p>Python 中提供了 <a href="https://docs.python.org/3/howto/descriptor.html" target="_blank" rel="noopener">Descriptor</a>的机制。在 Python 中，可以认为当访问对象的属性时，等价于调用对象的 <code>__get__()</code>和 <code>__set__()</code> 方法，因此我们可以覆盖这两个方法来修改访问的逻辑。</p><p>同样的，Kotlin 在定义<a href="https://kotlinlang.org/docs/reference/properties.html" target="_blank" rel="noopener">properties</a> 也可以自定义的 getter/setter 方法来修改属性访问的逻辑。</p><p>这里想说明的是，getter/setter 其实应该是默认实现，然后有需要时再覆盖，而不是每次都手工实现。</p><h2 id="社区与约定"><a class="header-anchor" href="#社区与约定">#</a>社区与约定</h2><p>也许你会问，封装其实叫什么名字都行，为什么非要叫 <code>getXXX</code> 及 <code>setXXX</code> 呢？这其实是<a href="http://download.oracle.com/otn-pub/jcp/7224-javabeans-1.01-fr-spec-oth-JSpec/beans.101.pdf?AuthParam=1522674989_0d7c790344741da888ed8c0e890ea7d5" target="_blank" rel="noopener">JavaBeans</a>里约定的（7.1 节）。甚至从某种角度来说 getter/setter 的目的也不是为了封装，而只是一个约定，使框架能识别 JavaBeans 中的 property。</p><p>在实际工作中你会发现，90% 以上的 getter/setter 在未来并不会被用来增加逻辑什么的。所以“封装”的作用理论上是好的，但实际被使用到的频率特别低，反而增加了许多无用的代码。</p><p>另一方面，随着使用 getter/setter 使用的增加，且由于绝大多数 getter/setter 并不会增加额外的逻辑，使得人们开始习惯于假设 getter/setter 不会有额外逻辑。所以如果你想在 setter 里加一些额外的逻辑时，反而要注意会不会让使用的人感到吃惊。</p><h2 id="写在最后"><a class="header-anchor" href="#写在最后">#</a>写在最后</h2><p>Getter/Setter 这个话题看上去似乎很简单，它的背后却有很多可以深究和思考的内容的。有人说 Getter 没关系，可怕的是 Setter；也有说现在lombok 这么方便，用Getter/Setter 有利无害；也有人说尽量避免使用 Getter/Setter。这些观点背后都藏着一些软件的设计思维。例如怎样设计类的接口，如何实现封装，这些都是后续需要学习思考的内容。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;java 有一个不成文的规定，如果要访问一个类的 private 字段，就需要写
getter/setter 方法。但我们在其它语言却很少见到类似的约定，为什么？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;它是“封装”的体现，对外隐藏了具体实现，允许之后对属性的访问注入新的逻辑（如验证逻辑
      
    
    </summary>
    
      <category term="QQA" scheme="http://lotabout.github.io/categories/QQA/"/>
    
    
      <category term="QQA" scheme="http://lotabout.github.io/tags/QQA/"/>
    
      <category term="java" scheme="http://lotabout.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>QQA: Rust 中 Send 与 Sync 有什么区别</title>
    <link href="http://lotabout.github.io/2018/QQA-send-vs-sync-in-rust/"/>
    <id>http://lotabout.github.io/2018/QQA-send-vs-sync-in-rust/</id>
    <published>2018-04-01T08:32:37.000Z</published>
    <updated>2018-07-29T02:13:50.117Z</updated>
    
    <content type="html"><![CDATA[<p><code>Send</code> 与 <code>Sync</code> 是两个十分相近的 trait，它们是一起保证了 Rust 的线程安全，它们又有什么异同点呢？</p><p>（Quick Question and Answer 系列旨在对小问题做简短解答）</p><a id="more"></a><ul><li><code>Send</code> 表示数据能安全地被 <code>move</code> 到另一个线程</li><li><code>Sync</code> 表示数据能在多个线程中被同时安全地访问</li></ul><p>这里“安全”指不会发生数据的竞争 (race condition)。</p><h2 id="send-代表没有数据共享"><a class="header-anchor" href="#send-代表没有数据共享">#</a>Send 代表没有数据共享</h2><p>数据如果被 move 到另一个线程里，它还安全吗？能正常使用吗？如果可以，则说它是<code>Send</code>。</p><p>反例： <a href="https://doc.rust-lang.org/std/rc/struct.Rc.html" target="_blank" rel="noopener">Rc</a>。我们知道 <code>Rc</code> 中保存了一个 reference count，记录有多少变量引用了当前的数据，当 reference count归 0 时才释放(drop)数据本身。现在如果我们把一个 <code>Rc</code> move 到另一个线程里，尽管是 move，<code>Rc</code> 的实现还是决定了不同线程里的 <code>Rc</code> 会指向同一个 reference count，这意味着不同的线程可能同时修改 reference count，而 <code>Rc</code> 内部并没有实现同步机制，因此是不安全的。</p><p>这里有一个推论：一个结构(Struct)如果不满足 <code>Send</code>，是不是意味着它的某个内部数据不满足<code>Sync</code> 呢？参考 <code>Rc</code> 的例子，就是内部的 reference count 不满足 Sync 。只是目前没有找到相关的证明。</p><h2 id="sync-代表同步"><a class="header-anchor" href="#sync-代表同步">#</a>Sync 代表同步</h2><p>如果多个线程同时访问某个数据，会不会产生竞争？如果还是安全的，我们就能说它是<code>Sync</code>。</p><p>反例：<a href="https://doc.rust-lang.org/std/cell/struct.RefCell.html" target="_blank" rel="noopener">RefCell</a>。它满足 <code>Send</code>，但不满足 <code>Sync</code>。 <code>RefCell</code> 不会与本线程的其它引用共享数据，所以被move 到其它线程是安全的。但如果有多个线程同时拥有 RefCell 的引用，并同时获取它的可变引用(mutable reference)并尝试修改它，则会产生竞争，亦即没有满足原子性。</p><h2 id="marker-trait"><a class="header-anchor" href="#marker-trait">#</a>marker trait</h2><p>最后要说的是，<code>Send</code> 和 <code>Sync</code> 都属于 <a href="https://doc.rust-lang.org/std/marker/index.html" target="_blank" rel="noopener">markertrait</a>，marker trait 的特点是不包含任何方法，所以为某个数据结构实现 marker trait 相当于人为告诉编译器，我实现的数据结构符合你的要求（如满足 <code>Send</code>, <code>Sync</code>），编译期间就放心吧。换句话说，编译器并无法检查你的实现到底是不是满足 <code>Send</code> 或 <code>Sync</code>，只能选择相信程序员的声明，如果程序员的实现有问题，只能程序员自己背锅了。</p><h2 id="参考"><a class="header-anchor" href="#参考">#</a>参考</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/24142191" target="_blank" rel="noopener">线程安全</a> Rust 线程安全的讲解</li><li><a href="https://doc.rust-lang.org/book/second-edition/ch16-03-shared-state.html" target="_blank" rel="noopener">Shared State Concurrency</a> 详细说明了 Rc 为什么不满足 Send</li><li><a href="https://manishearth.github.io/blog/2015/05/30/how-rust-achieves-thread-safety/" target="_blank" rel="noopener">How Rust Achieves Thread Safety</a> 对 Rust 中的 Send &amp; Sync 机制有比较详细的说明</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;Send&lt;/code&gt; 与 &lt;code&gt;Sync&lt;/code&gt; 是两个十分相近的 trait，它们是一起保证了 Rust 的线程安全，它
们又有什么异同点呢？&lt;/p&gt;
&lt;p&gt;（Quick Question and Answer 系列旨在对小问题做简短解答）&lt;/p&gt;
    
    </summary>
    
      <category term="QQA" scheme="http://lotabout.github.io/categories/QQA/"/>
    
    
      <category term="QQA" scheme="http://lotabout.github.io/tags/QQA/"/>
    
      <category term="rust" scheme="http://lotabout.github.io/tags/rust/"/>
    
  </entry>
  
  <entry>
    <title>QQA: 什么是 Servlet</title>
    <link href="http://lotabout.github.io/2018/QQA-What-is-servlet/"/>
    <id>http://lotabout.github.io/2018/QQA-What-is-servlet/</id>
    <published>2018-03-31T10:16:52.000Z</published>
    <updated>2018-07-29T02:13:50.117Z</updated>
    
    <content type="html"><![CDATA[<p>Servlet 没有标准的中文译名，我们会在学习 Java Web 编程（如 Spring）时遇到，你知道它是什么吗？</p><p>（Quick Question and Answer 系列旨在对小问题做简短解答）</p><h2 id="什么是-servlet"><a class="header-anchor" href="#什么是-servlet">#</a>什么是 Servlet</h2><p>英语里 <code>-let</code> 后缀代表“小型的”，如 “booklet” 是小册子，所以 'Servlet’可以认为是“小 Server”。</p><ul><li>狭义上，Servlet 是一个接口，定义在 <code>javax.servlet.Servlet</code>。</li><li>广义上，任何实现了 Servlet 接口的程序都可以叫作 Servlet</li></ul><h2 id="web-应用的结构"><a class="header-anchor" href="#web-应用的结构">#</a>Web 应用的结构</h2><p>假设我们要写一个 Web 应用，什么框架都不用，应该从何做起？一般来说，我们需要：</p><ol><li>监听 TCP 请求，并解析出 TCP 中的 HTTP 请求</li><li>有了 HTTP 请求，我们需要根据 HTTP 请求找到对应的函数来执行（路由）</li><li>实现函数中的 business 逻辑，例如从数据库里查询数据，做操作，生成返回的内容等等。</li><li>将返回的内容用 HTTP 请求包裹</li><li>用 TCP 将生成的 HTTP 请求返回</li></ol><p>我们发现 1,2,4,5 这些工作不管开发什么样的应用都要实现一次，那干脆单独抽出来做成模块。于是 1,5 就被做成了 web server ，如 tomcat；2,4就被独立成 web 框架，如Spring。</p><img src="/2018/QQA-What-is-servlet/servlet.svg" title="Structure of Web Application"><p>如上，Servlet 接口就是 web server 与框架间的通信协议，所以在学习 Web 框架的实现时容易遇到它。但写具体的应用时，因为框架已经屏蔽了这些细节，所以基本也不会用到 Servlet 的概念。</p><h2 id="参考"><a class="header-anchor" href="#参考">#</a>参考</h2><ul><li><a href="https://ruslanspivak.com/lsbaws-part1/" target="_blank" rel="noopener">Let’s Build A Web Server</a> Python 教程，从头写一个 Web Server，对理解 web server 的工作原理很有帮助</li><li><a href="https://www.mulesoft.com/cn/tcat/tomcat-servlet" target="_blank" rel="noopener">An Introduction to Tomcat Servlet Interactions</a> Tomcat 如何与 Servlet 进行交互</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Servlet 没有标准的中文译名，我们会在学习 Java Web 编程（如 Spring）时遇到，你知道它是什么吗？&lt;/p&gt;
&lt;p&gt;（Quick Question and Answer 系列旨在对小问题做简短解答）&lt;/p&gt;
&lt;h2 id=&quot;什么是-servlet&quot;&gt;&lt;a 
      
    
    </summary>
    
      <category term="QQA" scheme="http://lotabout.github.io/categories/QQA/"/>
    
    
      <category term="QQA" scheme="http://lotabout.github.io/tags/QQA/"/>
    
      <category term="java" scheme="http://lotabout.github.io/tags/java/"/>
    
      <category term="servlet" scheme="http://lotabout.github.io/tags/servlet/"/>
    
  </entry>
  
  <entry>
    <title>QQA: 如何启用 @Autowired</title>
    <link href="http://lotabout.github.io/2018/QQA-How-to-enable-autowire/"/>
    <id>http://lotabout.github.io/2018/QQA-How-to-enable-autowire/</id>
    <published>2018-03-30T18:19:53.000Z</published>
    <updated>2018-07-29T02:13:50.117Z</updated>
    
    <content type="html"><![CDATA[<p><code>@Autowired</code> 是 Spring 提供的一个注解，作用是自动装配 Bean 所需要的依赖。但<code>@Autowired</code> 只是告诉 Spring 当前的 Bean 依赖了其它的 Bean，那么如何让 Spring真正“启用”自动装配的功能呢？</p><p>（Quick Question and Answer 系列旨在对小问题做简短解答）</p><a id="more"></a><h2 id="错误示范"><a class="header-anchor" href="#错误示范">#</a>错误示范</h2><p>我们知道在 Spring 可以指定 Configuration Class 来提供 Bean，有时我们会看到下面的代码：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AServiceImpl</span> <span class="keyword">implements</span> <span class="title">AService</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> BService bService;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    AService(BService bService) &#123;</span><br><span class="line">        <span class="keyword">this</span>.bService = bService;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>与此同时在 Configuration Class 里：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">XConfig</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> BService <span class="title">bService</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> BServiceImpl();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> AService <span class="title">aService</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> AServiceImpl(bService());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>一方面，我们用 <code>@Autowired</code> 指定了 <code>AServiceImpl</code> 里要依赖 <code>BService</code>，别一方面我们在 Configuration Class 里 <strong>手工</strong> 创建了 <code>AService</code> 的 Bean，而这个依赖<code>bService</code> 也是我们自己指定的。因此 <code>@Autowired</code> 完全没有用……</p><h2 id="正确做法"><a class="header-anchor" href="#正确做法">#</a>正确做法</h2><p>从上面例子我们可以想到，要利用 <code>@Autowired</code>，我们要让框架来创建 Bean，那怎么做呢？</p><p><strong>(1) 为 Bean 加上注解 <code>@Component</code></strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AServiceImpl</span> <span class="keyword">implements</span> <span class="title">AService</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> BService bService;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    AService(BService bService) &#123;</span><br><span class="line">        <span class="keyword">this</span>.bService = bService;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>@Component</code> 的意义是告诉 Spring 当前的类是一个 Bean。</p><p><strong>(2) 在 Configuration Class 加上注解 <code>@ComponentScan</code></strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@ComponentScan</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">XConfig</span> </span>&#123;</span><br><span class="line">    <span class="comment">// no need to define @Bean here</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>@ComponentScan</code> 是告诉 Spring 在 <code>XConfig.java</code> 所有的 package （及子 package）中寻找带注解 <code>@Component</code> 的类并为其创建对应的 Bean。在创建的时候，Spring 就会查找类中是否有 <code>@Autowired</code> 注解的字段/方法/…… 并为其自动装配所需依赖。</p><h2 id="只为别人的类创建-bean"><a class="header-anchor" href="#只为别人的类创建-bean">#</a>只为别人的类创建 Bean</h2><p>一般来说 Configuration Class 里不需要手工创建 Bean。但有一些类可能不是你自己写的/管的，所以没有办法为它们加上 <code>@Autowired</code>，这时只得手工指定依赖并创建 Bean了。创建过程中，如果需要用到加了 <code>@Component</code> 的类，如下例的 <code>AService</code>，可以把它们作为参数，这样 Spring 在创建 OtherBean 时会自动装配 <code>AService</code>。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@ComponentScan</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">XConfig</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> OtherBean <span class="title">otherBean</span><span class="params">(AService aService)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> AServiceImpl(aService);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;@Autowired&lt;/code&gt; 是 Spring 提供的一个注解，作用是自动装配 Bean 所需要的依赖。但
&lt;code&gt;@Autowired&lt;/code&gt; 只是告诉 Spring 当前的 Bean 依赖了其它的 Bean，那么如何让 Spring
真正“启用”自动装配的功能呢？&lt;/p&gt;
&lt;p&gt;（Quick Question and Answer 系列旨在对小问题做简短解答）&lt;/p&gt;
    
    </summary>
    
      <category term="QQA" scheme="http://lotabout.github.io/categories/QQA/"/>
    
    
      <category term="QQA" scheme="http://lotabout.github.io/tags/QQA/"/>
    
      <category term="java" scheme="http://lotabout.github.io/tags/java/"/>
    
      <category term="autowire" scheme="http://lotabout.github.io/tags/autowire/"/>
    
  </entry>
  
  <entry>
    <title>WGAN 笔记</title>
    <link href="http://lotabout.github.io/2018/WGAN/"/>
    <id>http://lotabout.github.io/2018/WGAN/</id>
    <published>2018-03-29T18:25:30.000Z</published>
    <updated>2018-07-29T02:13:50.117Z</updated>
    
    <content type="html"><![CDATA[<p>Wasserstein GAN(WGAN) 解决传统 GAN 的训练难，训练过程不稳定等问题了。WGAN 的背后有强劲的数学支撑，因此要想理解这它的原理，需要理解许多数学公式的推导。这个笔记尽量尝试从直觉的角度来理解 WGAN 背后的原理。</p><h2 id="gan-的问题"><a class="header-anchor" href="#gan-的问题">#</a>GAN 的问题</h2><p>我们知道，GAN 的目的是训练一个生成器 G，使生成的数据的分布 $P_G$ 与真实数据的分布 $P_{data}$ 尽可能接近。为了衡量接近程度，GAN 使用 <a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence" target="_blank" rel="noopener">JSDivergence</a>来衡量。</p><p>从应用的角度，我们甚至不需要知道它是什么，我们只要知道，对于两个分布 $P_r$ 和$P_g$，如果它们不重合或重合的部分可以忽略，则它们的 JS 距离 $JS(P_r, P_g) =\log 2$ 是常数，用梯度下降时，产生的梯度会（近似）为 $0$。而在 GAN 的训练中，两个分布不重合或重合可忽略的情况 <strong>几乎总是出现</strong>，因此导致 GAN 的训练中</p><h2 id="wasserstein-gan"><a class="header-anchor" href="#wasserstein-gan">#</a>Wasserstein GAN</h2><p>依旧地，我们甚至不需要知道 <a href="https://en.wikipedia.org/wiki/Wasserstein_metric" target="_blank" rel="noopener">WassersteinDistance</a> 是什么，只需要知道它有着很好的性质，两个分布的差异都会反应在 Wasserstein Distance 上，因此，不会出现梯度消失的问题。</p><p>现在的问题是怎么计算它？答曰无法计算，但在 <a href="https://arxiv.org/pdf/1701.07875.pdf" target="_blank" rel="noopener">WassersteinGAN</a> 论文里证明了如下的事实：</p><p>$$W(P_{data},P_G)=\max_{D\in \text{1-Lipschitz}}\{E_{x\sim P_{data}}[D(x)]-E_{x\sim P_G}[D(x)]\}$$</p><p>在接下去之前我们先说说什么是 $\text{1-Lipschitz}$。如果一个函数 $f$ 满足下面式子：</p><p>$$||f(x_1)-f(x_2)||\le K||x_1-x_2||$$</p><p>我们就称它为 $K\text{-Lipschitz}$，当 $K=1$时，就是 $\text{1-Lipschitz}$。</p><p>在图像生成的 GAN 中，上式中的 $D(x)$ 可以认为是以图像为输入，输出图像的质量（是否接近真实图像）。那么我们可以找到两种类型的 $D$，一类变化剧烈，即赋予真实图像很大的值，而其它图像的值就很小（下图蓝色）；另一类则变化平缓（下图绿色）。相像一下，如果用变化剧烈的 D 作为判别器去训练生成器，则会倾向于生成和真实图像一模一样的图片，导致多样性不高。而 $\text{1-Lipschitz}$ 的作用就是限制 D 的变化要更平缓一些，是符合直觉的。</p><p><img src="http://friskit-blog.qiniudn.com/c/6c/2753647abb8b644a0720a17810f30.png" alt="Intuition for 1-Lipschitz"></p><p>于是我们现在的目标是找到一个函数 $D$ 满足 $\text{1-Lipschitz}$ 且让上面的式子最大。“最大化”倒是好说，我们不断用梯度上升，但怎么保证我们的判别器 D 满足$\text{1-Lipschitz}$ 呢？还是没有办法，但我们可以做一些 workaround。</p><h2 id="weight-clipping"><a class="header-anchor" href="#weight-clipping">#</a>Weight Clipping</h2><p>对于神经网络中的所有权重，在更新梯度后，我们事先选中某个常数 $c$， 做下面的操作：</p><ul><li>如果权重 $w &gt; c$，则赋值 $ w \leftarrow c$</li><li>如果权重 $w &lt; -c$，则赋值 $ w \leftarrow -c$</li></ul><p>直觉上，如果神经网络的权重都限制在一定的范围内，那么网络的输出也会被限定在一定范围内。换句话说，这个网络会属于某个 $K\text{-Lipschitz}$。当然，我们并不确定K是多少，并且这样的函数也不一定能使 $E_{x\sim P_{data}}[D(x)]-E_{x\simP_G}[D(x)]$ 最大化。</p><p>不管怎么说吧，这就是原版 WGAN 的方法，对 GAN 的具大提升。</p><h2 id="gradient-penalty"><a class="header-anchor" href="#gradient-penalty">#</a>Gradient Penalty</h2><p>新版的 WGAN 提出了不用 weight clipping，而用加惩罚项的方式，我们去优化下面这个目标：</p><p>$$W(P_{data},P_G)=\max_{D}\{E_{x\sim P_{data}}[D(x)]-E_{x\sim P_G}[D(x)]\underbrace{-\lambda\int_x\max(0,||\nabla_xD(x)||-1)dx}_{\text{regularization}}\}$$</p><p>为什么呢？因为如果 $D\in \text{1-Lipschitz}$，显然对于所有 $x$，我们有$||\nabla_xD(x)|| \le 1$。但同之前一样，我们无法穷举所有 $x$ 求积分，于是我们又用期望来近似它，于是有：</p><p>$$W(P_{data},P_G)=\max_{D}\{E_{x\sim P_{data}}[D(x)]-E_{x\sim P_G}[D(x)]\underbrace{-\lambda E_{x\sim P_{penalty}}[\max(0,||\nabla_xD(x)||-1)]}_{regularization}\}$$</p><p>那这里的 $P_{penalty}$ 又是什么？它代表的是输入 $x$ 的分布，那具体如何采样呢？新版 WGAN 是这样设计的：</p><ol><li>从真实数据 $P_{data}$ 中采样得到一个点</li><li>从生成器生成的数据 $P_G$ 中采样得到一个点</li><li>为这两个点连线</li><li>在线上随机采样得到一个点作为 $P_{penalty}$ 的点。</li></ol><p><img src="http://friskit-blog.qiniudn.com/2/32/1b101dceaaea8b8ccfd174b077713.png" alt="How to sample P_penalty"></p><p>为什么这么采样？直觉上，我们想将 $P_G$ “拉”向 $P_{data}$，于是希望 $D$ 在它们之间的这些数据上能更平缓地变化。而惩罚项就是为了保证 $D$ “平缓变化”的，于是正则项中的 $P_{penalty}$ 就在这些数据点上进行采样。</p><p>最后，实际中我们其实并不是用 $\max(0,||\nabla_xD(x)||-1)$ 这个惩罚项，而是用$(||\nabla_xD(x)||-1)^2$。也就是说，我们惩罚的目的不是让 $||\nabla_xD(x)||$尽可能小于1，而是要让它尽可能 <strong>等于</strong> 1。想象一个完美的判别器 $D$ 满足优化的目标，则在 $P_{data}$ 附近它要尽可能大，而在 $P_G$ 附近要尽可能小，也就是说$D$ 越斜越好，但由于 $||\nabla_xD(x)|| \le 1$，那么 $||\nabla_xD(x)||$ 只能是 1。所以，真正的优化目标如下：</p><p>$$W(P_{data},P_G)=\max_{D}\{E_{x\sim P_{data}}[D(x)]-E_{x\sim P_G}[D(x)]-\lambda E_{x\sim P_{penalty}}[(||\nabla_xD(x)||-1)^2]\}$$</p><h2 id="小结"><a class="header-anchor" href="#小结">#</a>小结</h2><p>GAN 的优化目标是 JS Divergence，它有许多缺点不利于 GAN 的训练。WassersteinDistance 是一个更好的距离度量，它最终可以转化为优化问题，我们需要找出一个判别器 $D$，并要求它满足 $\text{1-Lipschitz}$。实际使用时我们并做不到这一点，于是有两种方法来近似：weight clipping 和 gradient penalty。</p><h2 id="参考"><a class="header-anchor" href="#参考">#</a>参考</h2><ul><li><a href="https://www.youtube.com/watch?v=KSN4QYgAtao&amp;lc=z13kz1nqvuqsipqfn23phthasre4evrdo" target="_blank" rel="noopener">Improving GAN</a> 李宏毅老师的教学视频，深入浅出</li><li><a href="http://friskit.me/2017/07/10/ntu-gan-wgan/" target="_blank" rel="noopener">再读WGAN</a> 对李宏毅老师视频的文字总结，本文的一些公式和图的来源。</li><li><a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="noopener">令人拍案叫绝的Wasserstein GAN</a> 知乎神文，出色的 WGAN 总结</li><li><a href="https://vincentherrmann.github.io/blog/wasserstein/" target="_blank" rel="noopener">Wasserstein GAN and the Kantorovich-Rubinstein Duality</a> 图文并茂带你理解 Wasserstein Distance</li><li><a href="https://arxiv.org/pdf/1701.07875.pdf" target="_blank" rel="noopener">Wasserstein GAN</a> WGAN 原版论文，weight clipping 方法</li><li><a href="https://arxiv.org/pdf/1704.00028.pdf" target="_blank" rel="noopener">Improved Training of Wasserstein GANs</a> WGAN 新版论文，Gradient Penalty 方法</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Wasserstein GAN(WGAN) 解决传统 GAN 的训练难，训练过程不稳定等问题了。WGAN 的背后有强劲的数学支撑，因此要想理解这它的原理，需要理解许多数学公式的推导。这个笔记尽量尝试从直觉的角度来理解 WGAN 背后的原理。&lt;/p&gt;
&lt;h2 id=&quot;gan-
      
    
    </summary>
    
      <category term="Notes" scheme="http://lotabout.github.io/categories/Notes/"/>
    
    
      <category term="Machine Learning" scheme="http://lotabout.github.io/tags/Machine-Learning/"/>
    
      <category term="GAN" scheme="http://lotabout.github.io/tags/GAN/"/>
    
      <category term="WGAN" scheme="http://lotabout.github.io/tags/WGAN/"/>
    
  </entry>
  
  <entry>
    <title>ƒ-divergence GAN 笔记</title>
    <link href="http://lotabout.github.io/2018/f-divergence-GAN/"/>
    <id>http://lotabout.github.io/2018/f-divergence-GAN/</id>
    <published>2018-03-29T09:35:18.000Z</published>
    <updated>2018-07-29T02:13:50.121Z</updated>
    
    <content type="html"><![CDATA[<p>f-divergence GAN 是对 GAN 框架的理论统一，本文学习过程中的一些笔记，包括基本公式的推导和重要概念的理解。</p><p>学习资料是李宏毅老师 <a href="https://www.youtube.com/watch?v=KSN4QYgAtao&amp;lc=z13kz1nqvuqsipqfn23phthasre4evrdo" target="_blank" rel="noopener">关于 WGAN 的教学视频</a>视频里深入浅出地介绍了许多 GAN 的相关知识。不需要太多的数学基础就能听懂，强力推荐。</p><a id="more"></a><h2 id="gan-的基本思想"><a class="header-anchor" href="#gan-的基本思想">#</a>GAN 的基本思想</h2><p>有这样一个 GAN 的应用，它能用机器生成 <a href="https://qiita.com/mattya/items/e5bfe5e04b9d2f0bbd47" target="_blank" rel="noopener">动漫头像</a>。我们需要事先收集一些人类画师画的动漫头像，它们可以认为是图像空间(image page)里的某个分布 $P_{data}$。之后我们会尝试训练一个生成器 G，它能以随机噪声 $z$ 为输入，生成动漫头像，我们认为生成的头像满足分布 $P_G$。而训练的目标就是让 $P_G$尽可能地接近 $P_{data}$。换言之，我们希望机器生成的头像尽可能像人画出来的。</p><p><img src="https://blog.openai.com/content/images/2017/02/gen_models_diag_2.svg" alt="GAN Model">(图片来源：<a href="https://blog.openai.com/generative-models/" target="_blank" rel="noopener">https://blog.openai.com/generative-models/</a>)</p><p>理论上，如果我们有完美的 loss 函数，则训练生成器 G 和普通的神经网络没有任何区别。很可惜，我们并没有办法真正求出 $P_{data}$ 和 $P_G$，也因此我们不可能找到一个完美的 loss 函数来衡量“$P_{data}$ 与 $P_G$ 是否接近”。于是 GAN 的想法是，我们再训练一个判别器(Discriminator) 来尽量近似这个完美的 loss 函数。GAN 的基本结构如下：</p><img src="/2018/f-divergence-GAN/GAN.svg" title="GAN Model"><p>为了训练判别器 D，我们需要有正样本（动漫头像），也需要有负样本（非动漫头像）。正样本已经收集完毕，负样本哪里来呢？这就是 GAN 犀利的地方，它用生成器 G 生成的数据来作为负样本，用于训练判别器 D。而后我们得到一个更好的判别器 D 后，再用这个新的判别器 D 作为 loss 函数来训练 G 。于是我们能得到更好的生成器 G 以及判别器 D。</p><h2 id="gan-的算法"><a class="header-anchor" href="#gan-的算法">#</a>GAN 的算法</h2><p>算法的伪代码如下：</p><ul><li>初始化 D, G 的参数 $\theta_d$ 和 $\theta_g$</li><li>每一个迭代中：<ul><li>从真实数据的分布 $P_{data}(x)$ 中采样 $m$ 个样本 $\{x^1, x^2, \dots, x^m\}$</li><li>从先验的噪声分布 $P_{prior}(z)$ 中采样 $m$ 个样本 $\{z^1, z^2, \dots, z^m\}$</li><li>将噪声输入生成器 G，生成样本 $\{\tilde{x}^1, \tilde{x}^2, \dots, \tilde{x}^m\}, \tilde{x}^i = G(z^i)$</li><li>更新判别器 D 的参数，即最大化：<ul><li>$\tilde{V} = \frac{1}{m}\sum_{i=1}^m{\log D(x^i)} + \frac{1}{m}\sum_{i=1}^m{\log (1-D(\tilde{x}^i))}$</li><li>$\theta_d\leftarrow\theta_d+\eta\nabla\tilde{V}(\theta_d)$</li></ul></li><li>从先验的噪声分布 $P_{prior}(z)$ 中 <strong>再</strong> 采样 $m$ 个样本 $\{z^1, z^2, \dots, z^m\}$</li><li>更新生成器 D 的参数，即最小化：<ul><li>$\require{cancel}\tilde{V} = \cancel{\frac{1}{m}\sum_{i=1}^m\log D(x^i)} + \frac{1}{m}\sum_{i=1}^m\log (1-D(G(z^i)))$</li><li>$\theta_d\leftarrow\theta_d-\eta\nabla\tilde{V}(\theta_d)$</li></ul></li></ul></li></ul><p>这里的疑问是，为什么要最大化 $\tilde{V}$ 呢？换成其它的 $\tilde{V}$ 行不行？其实 ƒ-divergence GAN 就是要告诉我们，这么设计 $\tilde{V}$ 是有道理的，并且换成其它的 ƒ-divergence 也没有问题。</p><h2 id="ƒ-divergence"><a class="header-anchor" href="#ƒ-divergence">#</a>ƒ-divergence</h2><blockquote><p>In probability theory, an <a href="https://en.wikipedia.org/wiki/F-divergence" target="_blank" rel="noopener">ƒ-divergence</a> is a function$D_f(P||Q)$ that measures the difference between two probabilitydistributions $P$ and $Q$. Ithelps the intuition to think of the divergence as an average, weighted bythe function f, of the odds ratio given by $P$ and $Q$.</p></blockquote><p>给定两个分布 $P$ 和 $Q$，$p(x)$ 和 $q(x)$ 分别为对应样本的概率，ƒ-divergence 是一个这样的函数：</p><p>$$D_f(P||Q)=\int_xq(x)f(\frac{p(x)}{q(x)})dx$$</p><p>其中 $f$ 可以认为是 $D_f(P||Q)$ 的超参数，我们要求 $f$ 满足两点：(a) $f$ 是凸函数 (b) $f(1) = 0$</p><h3 id="为什么-d-f-可以衡量距离？"><a class="header-anchor" href="#为什么-d-f-可以衡量距离？">#</a>为什么 $D_f$ 可以衡量距离？</h3><p>如果 $P = Q$，则 $D_f(P||Q) = 0$。证明很简单，我们知道 $f(1) = 0$，所以当 $p(x) = q(x)$ 时，有：</p><p>$$D_f(P||Q)=\int_xq(x)\underbrace{f(\overbrace{\frac{p(x)}{q(x)}}^{=1})}_{=0}dx=0$$</p><p>而如果 $P \neq Q$，有 $D_f(P||Q) &gt; 0$。由于 $f$ 是凸函数，所以有：</p><p>$$\require{cancel}\begin{eqnarray}D_f(P||Q) &amp;=&amp; \int_xq(x)f(\frac{p(x)}{q(x)})dx \\&amp;\ge&amp; f(\int_x\cancel{q(x)}\frac{p(x)}{\cancel{q(x)}}dx)=f(1)=0\end{eqnarray}$$</p><p>因此，我们可以用 ƒ-divergence 来衡量两个分布的距离，如果两个分布相同，则 ƒ-divergence 为 0，而若分布不同，则 ƒ-divergence 大于 0。</p><h3 id="一些-ƒ-divergence"><a class="header-anchor" href="#一些-ƒ-divergence">#</a>一些 ƒ-divergence</h3><p>这里介绍的这些 divergence 我不知道是干什么用的。从应用的角度来说，似乎不明白也没什么关系。</p><p>当取 $f(x) = x \log x$ 时，我们就得到了 <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="noopener">KL divergence</a>：</p><p>$$D_f(P||Q)=\int_x q(x)\frac{p(x)}{q(x)}\log(\frac{p(x)}{q(x)})dx=\int_xp(x)\log(\frac{p(x)}{q(x)})dx$$</p><p>取 $f(x) = - \log x$ 时，我们就得到了 reverse KL-divergence:</p><p>$$D_f(P||Q)=\int_xq(x)(-\log(\frac{p(x)}{q(x)}))dx=\int_xq(x)\log(\frac{q(x)}{p(x)})dx$$</p><p>而取 $f(x) = (x-1)^2$ 时，得到的是 Chi Square divergence:</p><p>$$D_f(P||Q)=\int_x q(x)(\frac{p(x)}{q(x)}-1)^2dx = \int_x\frac{(p(x)-q(x))^2}{q(x)}dx$$</p><h3 id="ƒ-divergence-不是距离"><a class="header-anchor" href="#ƒ-divergence-不是距离">#</a>ƒ-divergence 不是距离</h3><p>很重要的一点 f-divergence 不是“距离”(<a href="https://en.wikipedia.org/wiki/Metric_(mathematics)" target="_blank" rel="noopener">metric</a>)，因为距离需要满足四个条件：</p><ol><li>$d(x, y) \ge 0$ 非负性</li><li>$d(x, y) = 0$ 当且仅当 $x = y$</li><li>$d(x, y) = d(y, x)$ 对称性</li><li>$d(x, z) \le d(x, y) + d (y, z)$ 三角不等式</li></ol><p>上面我们看到它满足前两个条件（严格来说 $D_f(P||Q) = 0$ 能不能推出 $P = Q$ 还不知道）。对剩下的条件，不同的 ƒ-divergence 有不同的情况。</p><p>例如 KL divergence 并不满足后对称性： $D_f(P||Q) \ne D_f(Q||P)$，也不满足三角不等式。证明我是肯定不会的，大家参考 <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Relation_to_metrics" target="_blank" rel="noopener">维基百科</a>。</p><p>而 <a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence" target="_blank" rel="noopener">Jensen–Shannon (JS)Divergence</a>就满足所有条件。一如既往，想看证明，请查看 <a href="http://www.math.ku.dk/~topsoe/ISIT2004JSD.pdf" target="_blank" rel="noopener">原论文</a>。</p><h2 id="fenchel-conjugate"><a class="header-anchor" href="#fenchel-conjugate">#</a>Fenchel Conjugate</h2><p>Conjugate 翻译是“共轭”，不明觉厉。对于每个凸函数，我们都可以 <strong>定义</strong> 一个它的共轭函数：</p><p>$$f^*(t) = \max_{x\in \mathbf{dom}(f)}\{xt-f(x)\}$$</p><p>对于理解 ƒ-divergence GAN 我们只需要知道对于常见常用的 $f$，我们可以定义并求出$f^*$ 的表达式就行了。但尝试理解 $f^*$ 涵义对我们还是有帮助的。</p><p>我们看到，当 $x$ 取特定值 $x_0$ 时 $g(t) = x_0t - f(x_0)$ 是一条直线。我们取$f(x) = x \log x$，x 取不同值时画出 $g(t)$ 的图像，如下所示：</p><img src="/2018/f-divergence-GAN/conjugate.png" title="Conjugate"><p>注意到 $f^*(t)$ 的定义为当 t 取某个值时，所有 $g(t)$ 的最大值。例如上图中，当 $t = 2$ 时，它与各直线的交点即为 $g(t)$ 的值，所以 $f^*(t)$ 的取值就是图点最高的点的值。</p><p>可以理解为，取不同的 x 值画出无穷多条直线 $g(t)$，这些直线的上边缘（上图红线）就是 $f^*(t)$。</p><p>最后，共轭函数有一个性质： $(f^* )^* = f$，也就是说：</p><p>$$f^*(t) = \max_{x\in \mathbf{dom}(f)}\{xt-f(x)\}\Longleftrightarrowf(x) = \max_{t\in \mathbf{dom}(f^*)} \{xt-f^*(x)\}$$</p><h2 id="ƒ-divergence-与-gan"><a class="header-anchor" href="#ƒ-divergence-与-gan">#</a>ƒ-divergence 与 GAN</h2><p>我们知道，GAN 的目的是训练生成器 G，使其产生的数据分布 $P_G$ 与真实数据的分布$P_{data}$ 尽可能小。换言之，如果我们用 ƒ-divergence 来表达 $P_G$ 与$P_{data}$ 的差异，则希望最小化 $D_f(P_{data}||P_G)$。注意到：</p><p>\begin{eqnarray}D_f(P||Q) &amp;=&amp; \int_xq(x)f(\frac{p(x)}{q(x)})dx \\&amp;=&amp; \int_xq(x)\left(\max_{t\in\mathbf{dom}(x^* )} \left\{\frac{p(x)}{q(x)}t-f^*(t)\right\}\right)dx\end{eqnarray}</p><p>于是乎，如果我们构造一个函数 $D(x) \in \mathbf{dom}(f^*)$，输入为 $x$，输出为$t$，则我们可以把上式的 $t$ 用 $D(x)$ 替代。但由于函数 $D$ 输出的 $x$ 不一定能使 $f$ 最大，所以有：</p><p>\begin{eqnarray}D_f(P||Q) &amp;\ge&amp; \int_xq(x)\left(\frac{p(x)}{q(x)}D(x)-f^*(D(x))\right)dx \\&amp;=&amp; \underbrace{\int_xq(x)D(x)dx - \int_xq(x)f^*(D(x))dx}_{M}\end{eqnarray}</p><p>因此，我们可以把求 $D_f(P||Q)$ 转化成一个最优化的问题：</p><p>\begin{eqnarray}D_f(P||Q) &amp;\approx&amp; \max_D\int_xp(x)D(x)dx-\int_xq(x)f^*(D(x))dx \\&amp;=&amp; \max_D\left\{\underbrace{E_{x\sim P}[D(x)]}_{\text{Samples from P}}- \underbrace{E_{x\sim Q}[f^*(D(x))]}_{\text{Samples from Q}} \right\}\end{eqnarray}</p><p>上面做了这一系列的转换，归根结底是因为实际总是中，我们并没办法求出 $p(x)$ 或$q(x)$，也没有办法穷举所有的 $x$，只能退而求其次求近似解。最终，我们把 GAN 的模型用数学公式表达即为：</p><p>$$\begin{align}G^*&amp;=\arg\min_GD_f(P_{data}||P_G) \\&amp;=\arg\min_G\max_D{E_{x\sim P_{data}}[D(x)]-E_{x\sim P_G}[f^*(D(x))]} \\&amp;= \arg\min_G\max_DV(G, D)\end{align}$$</p><p>当然，上面式子中的 $D$ 和我们在 GAN 模型里的判别器 D 还不一样。而且这个式子和我们之前说的 GAN 算法中的 $\tilde(V)$ 也是不同的。这是因为式子中的 $D$ 需要$D(x) \in \mathbf{dom}(f^*)$。所以我们需要选择合适的 $D$ 才能满足上式。这里我就不推导了，大家有兴趣可以看 <a href="https://arxiv.org/pdf/1606.00709.pdf" target="_blank" rel="noopener">原文</a>。</p><h2 id="小结"><a class="header-anchor" href="#小结">#</a>小结</h2><p>ƒ-divergence GAN 是对 GAN 模型的统一，对任意满足条件的 $f$ 都可以构造一个对应的 GAN。</p><p>GAN 的目的是训练生成器 D 使之生成的数据对应的分布 $P_G$ 与真实数据的分布$P_{data}$ 尽可能接近，即最小化 $D_f(P||Q)$。然而我们无法确切算出 $p(x)$ 及$q(x)$，因此我们通过 Conjugate 将求 $D_f(P||Q)$ 转变成一个优化问题，于是我们的目标变成找到一个合适的函数 $D$ 来逼近 $D_f(P||Q)$。</p><h2 id="参考"><a class="header-anchor" href="#参考">#</a>参考</h2><ul><li><a href="https://www.youtube.com/watch?v=KSN4QYgAtao&amp;lc=z13kz1nqvuqsipqfn23phthasre4evrdo" target="_blank" rel="noopener">Improving GAN</a> 李宏毅老师的教学视频，深入浅出</li><li><a href="https://arxiv.org/pdf/1606.00709.pdf" target="_blank" rel="noopener">f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization</a> f-GAN 原版论文</li><li><a href="http://friskit.me/2017/07/06/ntu-gan-basic/" target="_blank" rel="noopener">再读GAN</a> 对李宏毅老师视频的文字总结，包含对原版 GAN 的数学分析</li><li><a href="http://friskit.me/2017/07/10/ntu-gan-wgan/" target="_blank" rel="noopener">再读WGAN</a> 对李宏毅老师视频的文字总结，本文的很多公式的来源</li><li><a href="https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html#what-is-the-optimal-value-for-d" target="_blank" rel="noopener">From GAN to WGAN</a> 其中对 GAN 的一些问题有很好的阐述</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;f-divergence GAN 是对 GAN 框架的理论统一，本文学习过程中的一些笔记，包括基本公式的推导和重要概念的理解。&lt;/p&gt;
&lt;p&gt;学习资料是李宏毅老师 &lt;a href=&quot;https://www.youtube.com/watch?v=KSN4QYgAtao&amp;amp;lc=z13kz1nqvuqsipqfn23phthasre4evrdo&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;关于 WGAN 的教学视频
&lt;/a&gt;
视频里深入浅出地介绍了许多 GAN 的相关知识。不需要太多的数学基础就能听懂，强力
推荐。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://lotabout.github.io/categories/Notes/"/>
    
    
      <category term="Machine Learning" scheme="http://lotabout.github.io/tags/Machine-Learning/"/>
    
      <category term="GAN" scheme="http://lotabout.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>Back Propagation 笔记</title>
    <link href="http://lotabout.github.io/2018/Back-Propagation-Note/"/>
    <id>http://lotabout.github.io/2018/Back-Propagation-Note/</id>
    <published>2018-03-13T21:19:45.000Z</published>
    <updated>2018-07-29T02:13:50.085Z</updated>
    
    <content type="html"><![CDATA[<p>Michael Nielsen 的 <a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="noopener">深度学习</a> 文章里对 BP 算法有了相当全面的介绍，网上也有中文翻译版本。本文是自己学习的一些笔记，主要是抄一遍公式的证明来加强记忆。</p><h2 id="符号说明"><a class="header-anchor" href="#符号说明">#</a>符号说明</h2><img src="/2018/Back-Propagation-Note/NN.png" title="Decision Neural Network"><ul><li>$b_j^l$ 表示第 $l$ 层的第 $j$ 个节点对应的偏置</li><li>$w_{jk}^l$ 表示从第 $l-1$ 层的第 $k$ 个节点到 $l$ 层的 $j$ 个节点的连线的权重。</li><li>$z_j^l$ 表示第 $l$ 层的第 $j$ 个节点的加权输入，即 $z_j^l \equiv\sum_k{w_{kj}^l a_k^{l-1}} + b_j^l$</li><li>$a_j^l$ 表示第 $l$ 层的第 $j$ 个节点的激活输出，即 $a_j^l \equiv \sigma(z_j^l)$</li></ul><h2 id="bp-算法"><a class="header-anchor" href="#bp-算法">#</a>BP 算法</h2><ol><li>输入 $x$: 设输入层的激活值 $a^1 = x$。</li><li>前向传播：对于 $l = 2, 3, …, L$，计算 $z^l = w^l a^{l-1} + b^l$ 及 $a^l =\sigma(z^l)$。</li><li>计算 error $\delta^L = \nabla_a C \odot \sigma’(z^L)$。</li><li>反射传播错误：对于 $l = L-1, L-2, …, 2$ 计算$\delta^l = (( w^{l+1} )^T \delta^{l+1} ) \odot \sigma’(z^l)$</li><li>输出每层的梯度变化：$\nabla_{w^l} C = \delta^l (a^{l-1} )^T $ ， $\nabla_{b^l} C = \delta^l$</li></ol><p>如果算法需要计算多个样本 $x$ 对应的梯度变化，然后取平均时，可以输入 $X = [x_1,x_2, …, x_m]$，其中 $m$ 为样本数目。上面的算法不需要任何的修改，算法的输入变为：$\nabla_{b^l} C = [\nabla_{b^{l, 1}} C, \nabla_{b^{l, 2}} C, …\nabla_{b^{l, m}} C]$ ，$\nabla_{w^l} C = \sum_m{w^{l, m}}$ ，其中上标 ${l,m}$ 代表第 $m$ 个样本对应的第 $l$ 层。这可以认为是算法在多个样本下的矩阵形式。</p><h2 id="四个基本公式"><a class="header-anchor" href="#四个基本公式">#</a>四个基本公式</h2><p>矩阵形式：</p><p>\begin{eqnarray}\delta^L = \nabla_a C \odot \sigma’(z^L).\tag{BP1a}\end{eqnarray}</p><p>\begin{eqnarray}\delta^l = (( w^{l+1} )^T \delta^{l+1} ) \odot \sigma’(z^l )\tag{BP2a}\end{eqnarray}</p><p>\begin{eqnarray}\nabla_{b^l} C = \delta^l\tag{BP3a}\end{eqnarray}</p><p>\begin{eqnarray}\nabla_{w^l} C = \delta^l (a^{l-1} )^T\tag{BP4a}\end{eqnarray}</p><p>分量形式：</p><p>\begin{eqnarray}\delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma’(z^L_j)\tag{BP1}\end{eqnarray}</p><p>\begin{eqnarray}\delta^l_j = \sum_k w^{l+1}_{kj}  \delta^{l+1}_k \sigma’(z^l_j).\tag{BP2}\end{eqnarray}</p><p>\begin{eqnarray}\frac{\partial C}{\partial b^l_j} = \delta^l_j\tag{BP3}\end{eqnarray}</p><p>\begin{eqnarray}\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j.\tag{BP4}\end{eqnarray}</p><h2 id="公式推导"><a class="header-anchor" href="#公式推导">#</a>公式推导</h2><p>这里涉及很多变量和下标，这是理解神经网络“最大”的门槛了吧。下面我们要证明上面提到的四个公式，证明的过程基本是原文的翻译。</p><h3 id="bp1"><a class="header-anchor" href="#bp1">#</a>BP1</h3><p>下面我们先证明公式 BP1，我们要先定义 $\delta_j^l$：</p><p>$$\delta^l_j \equiv \frac{\partial C}{\partial z^l_j}$$</p><p>根据链式法则，由于 $C$ 是 $a_1^L, a_2^L, …$ 的函数，所以有</p><p>$$\delta^L_j = \sum_k{\frac{\partial C}{\partial a_k^L} \frac{\partial a_k^L}{\partial z_j^L}}$$</p><p>其中 $k$ 是输出层 L 的节点个数。当然，根据 $a_j^l$ 的定义我们知道，$a_j^l$ 完全取决于 $z_j^l$ 的值。这意味着当 $j \ne k$ 时，$\partial a_k^L/\partialz_j^L = 0$。于是上式又可以简化成：</p><p>$$\delta^L_j = \frac{\partial C}{\partial a_j^L} \frac{\partial a_j^L}{\partial z_j^L}$$</p><p>而由于 $a_j^L = \sigma(z_j^L)$，上式的第二项就可以用 $\sigma’(z_j^L)$ 替换，于是得到公式 BP1 ：</p><p>\begin{eqnarray}\delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma’(z^L_j)\tag{BP1}\end{eqnarray}</p><p>上式中 $\frac{\partial C}{\partial a_j^L}$ 取决于损失函数 $C$ 的选择，当 $C =\frac{1}{2}\sum_j(y_j - a_j^L )^2$ 时，有 $\partial C/\partial a_j^L = (a_j^L -y_j)$。</p><p>矩阵形式如下：</p><p>\begin{eqnarray}\delta^l= \begin{bmatrix}\delta_1^L \\\delta_2^L \\\vdots \\\delta_j^L\end{bmatrix}= \begin{bmatrix}\frac{\partial C}{\partial a_1^L} \\\frac{\partial C}{\partial a_2^L} \\\vdots \\\frac{\partial C}{\partial a_j^L}\end{bmatrix} \odot \begin{bmatrix}\sigma’(z_1^L) \\\sigma’(z_2^L) \\\vdots \\\sigma’(z_j^L)\end{bmatrix}= \nabla_a C \odot \sigma’(z^L)\tag{BP1a}\end{eqnarray}</p><h3 id="bp2"><a class="header-anchor" href="#bp2">#</a>BP2</h3><p>类似的，我们从定义出发：</p><p>$$\delta_j^l = \frac{\partial C}{\partial z_j^l}$$</p><p>类似上一节，$C$ 可以认为是任意一层的所有参数 $z_1^l, z_2^l, …$ 的复合函数，因此根据链式法则：</p><p>$$\delta_j^l= \frac{\partial C}{\partial z_j^l}= \sum_k{\frac{\partial C}{\partial z_k^{l+1}} \frac{\partial z_k^{l+1}}{\partial z_j^l}}= \sum_k{\delta_k^{l+1} \frac{\partial z_k^{l+1}}{\partial z_j^l}}$$</p><p>根据 $z_k^{l+1}$ 的定义，我们又有：</p><p>$$z_k^{l+1} = \sum_j{w_{kj}^{l+1} a_j^l + b_k^{l+1 }} = \sum_j{w_{kj}^{l+1} \sigma(z_j^l )+b_k^{l+1}}$$</p><p>对 $z_j^l$ 求导，注意到只有当 $z_j$ 匹配时导数才不为零，所以有 ：</p><p>$$\frac{\partial z_k^{l+1}}{\partial z_j^l} = w_{kj}^{l+1}\sigma’ (z_j^l)$$</p><p>最后代入 $\delta_j^l$ 的式子，即为公式 BP2：</p><p>\begin{eqnarray}\delta^l_j = \sum_k w^{l+1}_{kj}  \delta^{l+1}_k \sigma’(z^l_j).\tag{BP2}\end{eqnarray}</p><p>矩阵形式如下：</p><p>\begin{eqnarray}\delta^l&amp;=&amp; \begin{bmatrix}\delta_1^l \\\delta_2^l \\\vdots \\\delta_j^l\end{bmatrix}= \begin{bmatrix}w_{11}^{l+1} &amp; w_{21}^{l+1} &amp; \dots &amp; w_{k1}^{l+1} \\w_{12}^{l+1} &amp; w_{22}^{l+1} &amp; \dots &amp; w_{k2}^{l+1} \\\dots &amp; \dots &amp; \dots &amp; \dots \\w_{1j}^{l+1} &amp; w_{2j}^{l+1} &amp; \dots &amp; w_{kj}^{l+1}\end{bmatrix} \begin{bmatrix}\delta_1^{l+1} \\\delta_2^{l+1} \\\vdots \\\delta_k^{l+1} \\\end{bmatrix} \odot \begin{bmatrix}\sigma’(z_1^L) \\\sigma’(z_2^L) \\\vdots \\\sigma’(z_j^L)\end{bmatrix} \\&amp;=&amp; (( w^{l+1} )^T \delta^{l+1} ) \odot \sigma’(z^l )\tag{BP2a}\end{eqnarray}</p><h3 id="bp3"><a class="header-anchor" href="#bp3">#</a>BP3</h3><p>由于 $b_j^l$ 的作用于 $z_j^l$ 的作用基本一致，所以对于 BP2 的证明几乎可以直接对 $b_j^l$ 使用。$C$ 可以认为是任意一层的所有参数 $z_1^l, z_2^l, …$ 的复合函数，因此根据链式法则：</p><p>$$\frac{\partial C}{\partial b_j^l}= \sum_k{\frac{\partial C}{\partial z_k^{l+1}} \frac{\partial z_k^{l+1}}{\partial b_j^l}}= \sum_k{\delta_k^{l+1} \frac{\partial z_k^{l+1}}{\partial b_j^l}}$$</p><p>根据 $z_k^{l+1}$ 的定义，我们又有：</p><p>$$z_k^{l+1} = \sum_j{w_{kj}^{l+1} a_j^l + b_k^{l+1 }} = \sum_j{w_{kj}^{l+1} \sigma(z_j^l )+b_k^{l+1}}$$</p><p>对 $b_j^l$ 求导，注意到只有当 $z_j$ 匹配时导数才不为零，所以有 ：</p><p>$$\frac{\partial z_k^{l+1}}{\partial b_j^l}= w_{kj}^{l+1} \frac{\partial \sigma (z_j^l )}{\partial z_j^l} \frac{\partial z_j^l }{\partial b_j^l }= w_{kj}^{l+1}\sigma’ (z_j^l)$$</p><p>代入公式，有：</p><p>\begin{eqnarray}\frac{\partial C}{\partial b_j^l} = \sum_k w^{l+1}_{kj}  \delta^{l+1}_k \sigma’(z^l_j) = \delta^l_j\tag{BP3}\end{eqnarray}</p><p>矩阵形式如下：</p><p>\begin{eqnarray}\nabla_{b^l} C = \delta\tag{BP3a}\end{eqnarray}</p><h3 id="bp4"><a class="header-anchor" href="#bp4">#</a>BP4</h3><p>证明过程中先运用链式法则引入 $z_j^l$，之后代入 $z_j^l = \sum_k{w_{kj}^l a_k^{l-1}} + b_j^l$ 求导即可。</p><p>\begin{eqnarray}\frac{\partial C}{\partial w_{kj}^l} &amp;=&amp; \sum_i{\frac{\partial C}{\partial z_i^l} \frac{\partial z_i^l}{\partial w_{kj}^l}} \\&amp;=&amp; \frac{\partial C}{\partial z_j^l} \frac{\partial z_j^l}{\partial w_{kj}^l} = \delta_j^l \frac{\partial z_j^l}{\partial w_{kj}^l} \\&amp;=&amp; \delta_j^l \Big( \sum_i{\frac{\partial w_{ij}^l a_i^{l-1}}{\partial w_{kj}^l}} + \frac{\partial b_j^l}{\partial w_{kj}^l} \Big) \\&amp;=&amp; \delta_j^l a_k^{l-1} = a_k^{l-1} \delta_j^l\tag{BP4}\end{eqnarray}</p><p>矩阵形式如下（为了方便观看，把 $\partial C/\partial w_{kj}^l$ 与成了 $w_{kj}^l$）：</p><p>\begin{eqnarray}\nabla_{w^l} C= \begin{bmatrix}w_1^l \\w_2^l \\\dots \\w_j^l\end{bmatrix}&amp;=&amp; \begin{bmatrix}w_{11}^l &amp; w_{21}^l &amp; \dots &amp; w_{k1}^l \\w_{12}^l &amp; w_{22}^l &amp; \dots &amp; w_{k2}^l \\\dots &amp; \dots &amp; \dots &amp; \dots \\w_{1j}^l &amp; w_{2j}^l &amp; \dots &amp; w_{kj}^l\end{bmatrix} \\&amp;=&amp; \begin{bmatrix}a_1^{l-1} \delta_1^l &amp; a_2^{l-1} \delta_1^l &amp; \dots &amp; a_k^{l-1} \delta_1^l \\a_1^{l-1} \delta_2^l &amp; a_2^{l-1} \delta_2^l &amp; \dots &amp; a_k^{l-1} \delta_2^l \\\dots &amp; \dots &amp; \dots &amp; \dots \\a_1^{l-1} \delta_j^l &amp; a_2^{l-1} \delta_j^l &amp; \dots &amp; a_k^{l-1} \delta_j^l\end{bmatrix} \\&amp;=&amp; \begin{bmatrix}\delta_1^{l} \\\delta_2^{l} \\\vdots \\\delta_j^{l}\end{bmatrix} \begin{bmatrix}a_1^{l-1} &amp; a_2^{l-1} &amp; \dots &amp; a_k^{l-1}\end{bmatrix} \\&amp;=&amp; \delta^l (a^{l-1} )^T\tag{BP4a}\end{eqnarray}</p><h2 id="代码实现"><a class="header-anchor" href="#代码实现">#</a>代码实现</h2><p>可以在<a href="https://gist.github.com/lotabout/7a98d62caa4b0e7084ee0e85e79a5fe4" target="_blank" rel="noopener">lotabout/neural-network.py</a>中找到，代码在原文的基础之上实现了 mini batch 的矩阵运算。</p><h2 id="参考资料"><a class="header-anchor" href="#参考资料">#</a>参考资料</h2><ul><li><a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="noopener">http://neuralnetworksanddeeplearning.com/chap2.html</a> 本文的主要参考资料</li><li><a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Backprop/</a> 通俗讲解了 BP 的基本数学原理</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Michael Nielsen 的 &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap2.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;深度学习
&lt;/a&gt; 文章里对 BP 算法有了相当全面
      
    
    </summary>
    
      <category term="Notes" scheme="http://lotabout.github.io/categories/Notes/"/>
    
    
      <category term="Machine Learning" scheme="http://lotabout.github.io/tags/Machine-Learning/"/>
    
      <category term="Neural Network" scheme="http://lotabout.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归实验</title>
    <link href="http://lotabout.github.io/2018/Logistic-Regression-Notes/"/>
    <id>http://lotabout.github.io/2018/Logistic-Regression-Notes/</id>
    <published>2018-03-10T20:40:40.000Z</published>
    <updated>2018-07-29T02:13:50.093Z</updated>
    
    <content type="html"><![CDATA[<p>最近在看 Andrew Ng 老师的 <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">机器学习课程</a>，这篇文章试图通过编程的方式，一步步实验课程中的知识点，验证其中的一些结论，从而加深对逻辑回归的理解。</p><p>全文代码在 <a href="https://gist.github.com/lotabout/94c68304f23d0e0c06ad12a1334462cd" target="_blank" rel="noopener">lotabout/logistic-regression-experiment.py</a></p><h2 id="sigmoid-函数的作用"><a class="header-anchor" href="#sigmoid-函数的作用">#</a>Sigmoid 函数的作用</h2><p>在课程的引言里，给了一个二分类的任务，根据肿瘤的大小来判断它是不是恶性肿瘤。</p><p>首先，我们可以建立一个线性回归的模型 $h_\theta(X) = \theta^TX$。然后我们决定当 $h_\theta(X) &gt;= 0.5$ 时认为是恶性肿瘤，反之则认为不是恶性。</p><img src="/2018/Logistic-Regression-Notes/tumor-size.svg" title="Linear Regression for Tumorsize-maglignant"><p>但是这个模型是有问题的，当我们考虑更多的数据点时，线性回归模型可能就不再适用。例如，我们增加了几个数据后重新进行线性回归，依旧选定 0.5 作为分界线，这次，数据点 A 就被错误归类为“非恶性”。</p><img src="/2018/Logistic-Regression-Notes/tumor-size-wrong.svg" title="Linear Regression for Tumorsize-maglignant Wrong classification"><p>课程里说，在这个例子中使用线性回归的一个问题是，线性回归的结果最终会小于 <code>0</code>或超过 <code>1</code>，而在二分类任务里，本质上期望的值只有 <code>0</code> 和 <code>1</code>。线性模型的输出会随着输入的增大而增大，但数据的类别并不会随输入的增大而无限制增大。例如当tumor size 不断增大时，线性回归的模型输入会最终大于 <code>1</code>，而实际上类别的取值不会超过 <code>1</code>，因此线性模型与分类问题是不相符的。</p><p>也因此，Sigmoid 函数的作用可以认为是对“线性回归”的一个改进，使得当输入无限增大时，模型的输出不会无限增大。最终的模型是一条曲线：</p><img src="/2018/Logistic-Regression-Notes/tumor-size-sigmoid.svg" title="Linear Regression for Tumorsize-maglignant Sigmoid"><p>由于 sigmoid 函数的在 $-\infty$ 处几乎为 0，而 $\infty$ 处几乎为 1，因此在学习参数时两端的数据几乎不起作用。于是焦点就集中在两个类别的分界线，一个是sigmoid 函数的位移，一个是倾斜程度，如图：</p><img src="/2018/Logistic-Regression-Notes/tumor-size-sigmoid-choices.svg" title="Linear Regression for Tumorsize-maglignant Sigmoid Choices"><p>如果我们最终决策时只关心它的输出是大于还是小于 <code>0.5</code>，其实曲线的倾斜程度也没什么作用。</p><h2 id="逻辑回归是线性模型？"><a class="header-anchor" href="#逻辑回归是线性模型？">#</a>逻辑回归是线性模型？</h2><p>首先我们来认识一下什么是决策边界。在二分类中，决策边界是数据集上的一个超平面，用来划分两个类别。例如上节的例子中，输入是一维 “tumor size”，因此决策边界是一个点，在线性回归模型或逻辑回归模型中，我们都可以认为是决策边界是 y 取 0.5 时对应的 “tumor size” 的值。</p><p>虽然 Sigmoid 函数是 <code>S</code> 形，但模型是不是“线性”并不是看输出结果的。从数学的角度看，逻辑回归的输出 $h_\theta(x) = sigmoid(\theta^Tx)$ 是全取决于输入 $x$ 的线性组合 $\theta^Tx$ ，因此逻辑回归是线性模型。我们再来直观感受一下，下面是输入是二维时逻辑回归的输出平面：</p><img src="/2018/Logistic-Regression-Notes/Sigmoid-decision-boundary-3D.png" title="Sigmoid Decision Boundary 3D"><p>我们可以看到模型的输出是一个曲面。但模型的决策边界是模型输出为 0.5 时对应的输入。即上图中投影后的两个区域的边界。已经能看出这个边界是一条直线，我们将这个投影放在平面上：</p><img src="/2018/Logistic-Regression-Notes/Sigmoid-decision-boundary-2D.svg" title="Sigmoid Decision Boundary 2D"><p>要是有兴趣可以计算一下，它对应了 $\theta^Tx = sigmoid^{-1}(0.5)$ 这个平面。</p><h2 id="如何实现非线性？"><a class="header-anchor" href="#如何实现非线性？">#</a>如何实现非线性？</h2><p>上面我们知道逻辑回归模型是线性模型，这意味着，如果样本的分布是非线性的，则采用逻辑回归的模型是没办法正确进行分类的，如下面这样的样本（数据取自<a href="https://github.com/HanXiaoyang/ML-examples/blob/master/logistic_regression/data2.txt" target="_blank" rel="noopener">HanXiaoyang/ML-examples</a>）：</p><img src="/2018/Logistic-Regression-Notes/circular-data.svg" title="Circular Data"><p>这时，模型本身的能力已经没办法提高了，于是我们需要从 <strong>输入</strong> 下手。我们观察数据的分布比较接近椭圆形，因此我们手工地多加入一些数据: $x_1^2, x_1x_2, x_2^2,x_1^3, x_1^2x_2, … x_2^6$。通过人工地加入更多的数据，我们赋予了逻辑回归拟合非线性数据的能力，下面是我们用这些数据进行的一次训练：</p><img src="/2018/Logistic-Regression-Notes/circular-data-boundary-2D.svg" title="Circular Data boundary 2D"><p>可以看到，两类数据被清楚地分开了，但由于引入过多的维数，也产生了过拟合的现象。这里要强调的是，逻辑回归如果要处理非线性的数据， <strong>一定需要对输入进行预处理</strong>。例如加入高次项，来引入非线性的能力。换句话说，还是要做“特征工程”。</p><h2 id="损失函数"><a class="header-anchor" href="#损失函数">#</a>损失函数</h2><p>逻辑回归的 Hypothesis 为: $h_\theta(x) = 1 / (1 + e^{- \theta^T x})$，我们的任务是找到一个 “合适” 的 $\theta$ 来使这个 hypothesis 尽可能地解决我们的问题。例如分类任务，我们希望决策边界能最大程度将数据区分开。那么数学上怎么表达这种需求呢？</p><p>在线性回归中，一般采用均方误差用来评价一个 $\theta$ 的好坏：</p><p>$$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}{\frac{1}{2} (h_\theta(x^{(i)} ) - y^{(i)} )^2}$$</p><p>即 $J(\theta)$ 越小，认为 $\theta$ 越好。那为什么不直接把逻辑回归的$h_\theta(x)$ 代入均方误差呢？原因是这样产生的 $J(\theta)$ 是非凸函数(non-convex)。我们举个例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">samples = [(<span class="number">-5</span>, <span class="number">1</span>), (<span class="number">-20</span>, <span class="number">0</span>), (<span class="number">-2</span>, <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(theta, x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span> + math.e**(- theta*x))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta)</span>:</span></span><br><span class="line">    diffs = [(sigmoid(theta, x) - y) <span class="keyword">for</span> x,y <span class="keyword">in</span> samples]</span><br><span class="line">    <span class="keyword">return</span> sum(diff * diff <span class="keyword">for</span> diff <span class="keyword">in</span> diffs)/len(samples)/<span class="number">2</span></span><br><span class="line"></span><br><span class="line">X = np.arange(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">0.01</span>)</span><br><span class="line">Y = np.array([cost(theta) <span class="keyword">for</span> theta <span class="keyword">in</span> X])</span><br><span class="line">plt.plot(X, Y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/2018/Logistic-Regression-Notes/non-convex.svg" title="Square Cost function is non-convext"><p>可以看出这个损失函数是非凸的，局部最小值不等于全局最小值，因此使用梯度下降法难以求解。因此逻辑回归模型使用如下的损失函数，至于为它为什么是凸的，这里就不证明了：</p><p>$$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}{ Cost( h_\theta (x^{(i)}) , y)} \\Cost( h_\theta (x) , y) = \begin{cases}-\log(h_\theta(x)), &amp; \text{if}\ y = 1 \\-\log(1 - h_\theta(x)), &amp; \text{if}\ y = 0\end{cases}$$</p><p>写成统一的形式：</p><p>$$J(\theta) = - \frac{1}{m}\Big[\sum_{i=1}^{m}{ y^{(i)} \log h_{\theta} ( x^{(i)} ) +(1-y^{(i)} ) \log (1-h_\theta(x^{(i)} ) )}\Big]$$</p><p>那么损失函数是如何影响决策的呢？首先，损失函数是对 $h_\theta(x)$ 给出错误结论的惩罚。因此损失越小，一般就认为 $h_\theta(x)$ 的结论就越正确。而上面这个式子意味着，损失越小，最后得到的 $h_\theta(x)$ 曲面会越“贴近”数据点，换言之会“越陡”：</p><img src="/2018/Logistic-Regression-Notes/cost-function-3D.svg" title="Cost function 3D"><p>这幅图中，$J(\theta_{blue}) &lt; J(\theta_{green})$，即蓝色曲面对应的 $\theta$ 的损失要小于绿色曲面对应的 $\theta$ 值。可以看到，损失小的蓝色曲面更陡。</p><p>损失函数对决策边界有何影响？我们取 $h_\theta(x) = 0.5$ 的决策边界，可以看到决策边界也有略微的不同：</p><img src="/2018/Logistic-Regression-Notes/cost-function-2D.svg" title="Cost function 2D"><p>但由于这两个 $\theta$ 都能把这两组数据区分开，因此它们并没有特别大的差别。这里博主猜想，逻辑回归的训练中，前几个迭代应该就能快速地制定出决策边界，接下来一些迭代的作用应该就是让 $h_\theta(x)$ “更陡”，一味追求损失更小究竟对决策边界有帮助吗？</p><p>小结一下，如何决定模型的损失函数？一是损失函数要正确评价参数，使损失更小的参数对解决问题更有利；另一方面，受限于优化手段，要求损失函数能求解。当然一些常用的模型损失函数也大致确定了。</p><h2 id="正则化"><a class="header-anchor" href="#正则化">#</a>正则化</h2><p>如果出现过拟合，可以考虑去掉一些特征，但这些特征可能包含了重要信息，并不合适直接去掉。另一个方法就是加正则项。在逻辑回归中，加了正则项的损失函数如下：</p><p>$$J(\theta) = - \frac{1}{m}\Big[\sum_{i=1}^{m}{ y^{(i)} \log h_{\theta} ( x^{(i)} ) + (1-y^{(i)} ) \log (1-h_\theta(x^{(i)} ) )}\Big] + \lambda \sum_{j=1}^{m}{( \theta_j^2 )}$$</p><p>注意公式最后增加的部分： $\lambda \sum_{j=1}^{m}{( \theta_j^2)}$ 就是所谓的“正则项”。它的存在意味着我们在优化时，不仅仅想让 $J(\theta)$ 尽可能小，同时也想保证 $\theta$ 也尽可能小。由此来减少过拟合。下面的图是我们取不同 $\lambda$ 时训练后的决策边界：</p><img src="/2018/Logistic-Regression-Notes/regularization.svg" title="Regularization"><p>可以看到，随着 $\lambda$ 的增加，决策边界越来越平滑，但同时决策边界的准确性下降了。这是因为 $\lambda$ 过大时，优化 $J(\theta)$ 的主要目标变成了优化正则项而不是损失函数。</p><p>最后，我们看看这四个 $\lambda$ 的选择下，求得的 $\theta$ 的 $l_2$ 范数（即$\theta$ 代表的向量的长度。</p><table><thead><tr><th>$\lambda$</th><th>norm of $\theta$</th></tr></thead><tbody><tr><td>0</td><td>305.21</td></tr><tr><td>0.001</td><td>75.36</td></tr><tr><td>0.01</td><td>29.71</td></tr><tr><td>0.1</td><td>10.02</td></tr></tbody></table><p>这也说明了如果我们增大正则项的比重，训练得到的 $\theta$ 会更小。而更小的$\theta$ 通常意味着过拟合程度的减小，因为高次项所占的比重也会变小。</p><h2 id="特征缩放"><a class="header-anchor" href="#特征缩放">#</a>特征缩放</h2><p>经常的，数据的各个特征的尺度不同，而如果差异很大，则会影响训练模型的速度。因此在训练前我们常常需要把各个特征缩放到同一尺度上。下图是用梯度下降法训练上节数据的学习曲线，可以看到差别相当明显。</p><img src="/2018/Logistic-Regression-Notes/learning-rate-scaled.svg" title="Learning Curve Scaled vs Not-scaled"><h2 id="偏置"><a class="header-anchor" href="#偏置">#</a>偏置</h2><p>在训练的时候，其实是需要手工加上偏置项 <code>1</code> 的，例如两维样本 $[x_1, x_2]$ 就需要扩充成 $[1, x_1, x_2]$，这个 <code>1</code> 就是所谓的偏置项。这样训练出来的 $\theta$就会比维度多一。偏置项有什么用呢？</p><p>让我们考虑一维数据，此时我们的 $h(x) = \theta \times x$，它代表了一条直线，且<strong>必须过原点</strong> 。这样，如果样本偏离了原点，这条直线（决策边界）就没有办法很好地将数据分开。而加了偏置项，相当于模型变成 $h(x) = \theta_1 \times x +\theta_0$，就能表示任意的直线了。下面我们用 iris 数据分别训练了带偏置和不带偏置的两个模型：</p><img src="/2018/Logistic-Regression-Notes/bias-or-not.svg" title="Decision Boundary with Bias or Not"><p>我们看到，不带偏置 (bias) 的模型的决策边界过原点，因此没有办法将数据很好地划分开，而带偏置的模型则能很好地对数据进行划分。换句话说，偏置提供了平移的能力。</p><h2 id="参考"><a class="header-anchor" href="#参考">#</a>参考</h2><ul><li><a href="http://blog.csdn.net/han_xiaoyang/article/details/49123419" target="_blank" rel="noopener">机器学习系列(1)_逻辑回归初步</a></li><li><a href="http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex5/ex5.html" target="_blank" rel="noopener">Exercise 5: Regularization</a> 正则化的习题，有许多数学上的讲解</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近在看 Andrew Ng 老师的 &lt;a href=&quot;https://www.coursera.org/learn/machine-learning&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;机器学习课程
&lt;/a&gt;，这篇文章试图通过编程的方式，一步步
      
    
    </summary>
    
      <category term="Knowledge" scheme="http://lotabout.github.io/categories/Knowledge/"/>
    
    
      <category term="Machine Learning" scheme="http://lotabout.github.io/tags/Machine-Learning/"/>
    
      <category term="Logistic" scheme="http://lotabout.github.io/tags/Logistic/"/>
    
      <category term="Regression" scheme="http://lotabout.github.io/tags/Regression/"/>
    
  </entry>
  
  <entry>
    <title>性能优化三件套</title>
    <link href="http://lotabout.github.io/2018/performance-optimization/"/>
    <id>http://lotabout.github.io/2018/performance-optimization/</id>
    <published>2018-03-05T10:56:27.000Z</published>
    <updated>2018-07-29T02:13:50.129Z</updated>
    
    <content type="html"><![CDATA[<p>提高性能最怕是没有方向，没有头绪的时候不妨试试三件套：并行、异步、加缓存。</p><h2 id="并行"><a class="header-anchor" href="#并行">#</a>并行</h2><p>并行比较容易理解，一个人做不完的事，多喊几个人一起做。从算法的角度，“并行”相当于“分治”。</p><p>早期 CPU 是单核的，后来单核性能提高不了，就采用多核；早期单进程、单线程，后期用多进程、多线程；早期单台机器，后来多机分布式。这些可以说都是“并行”的思想。所以如果某个逻辑实在难以提高的时候，可以考虑“并行”来处理。</p><p>当然，并行也会引入新的问题，最常见的就是同步问题，不论是单机上的多线程，还是多机之间的数据同步，都是程序员很头疼的问题。还有就是数据之间的拷贝和汇总，也都需要额外的开销。因此引入并行的得与失，需要具体问题具体分析（其实任何策略都需要）。</p><h2 id="异步"><a class="header-anchor" href="#异步">#</a>异步</h2><p>“异步”和“缓存”都可以看作是 2/8 定理的延伸。也就是说 20% 的内容，虽然占少数，却是最重要的，而剩余的 80% ，虽然占多数，却是次要的。那么“异步”就是说当前有更重要的事要做，其它的不那么重要的可以在后台慢慢做，做完了我再处理。</p><p>例如现在几乎所有的网站都用 AJAX 来异步加载资源。换句话说，当用户打开一个网页时，它想要看到“主页面”的内容，因此这些内容是最重要的。而另外一些内容，例如对话框的内容和样式，在用户做一些操作之前，都是用不到的。因此我们可以异步加载对话框需要的资源，而把主要的精力放在加载和渲染“主页面”。</p><p>而在后端也有类似的场景。例如写数据库，一般情况下我们可能并不急着用写入的结果，那么“等待写入完成”就是一个次要的事情，因此可以异步执行写入操作，而先执行一些更重要的操作。</p><p>在现实生活中其实也类似，例如烧水，我们不急着喝就不需要等着它，可以先做一些其它的事，等用空时再查看水烧开没有。</p><p>当然，异步在某种程度上要求“并行”，需要异步进行的操作一般不能占用计算资源，例如或 IO 操作。或者需要指定其它的计算资源，例如用其它的机器执行计算。</p><p>异步也会极大增加编程的难度，想必不需要我多说吧。如果你觉得还行，那么考虑下异步加上并行吧。</p><h2 id="加缓存"><a class="header-anchor" href="#加缓存">#</a>加缓存</h2><p>“异步”可以认为是 2/8 定理在“计算”上的应用，那么缓存就是 2/8 定理在“数据”上的应用。即数据的重要程度是不同的，总有一些数据会被频繁地访问，而另一些数据则基本无人问津。</p><p>缓存已经被大量应用。在计算机的组成中，就有一级缓存、二级缓存、内存到硬盘这样的层级存储结构，一级缓存速度最快但容量小，硬盘容量大但速度慢；在代码中我们也会用HashMap 来自己做一些缓存；在高并发的压力之下，人们也发明了像 redis 等的快速缓存。</p><p>因此，如果发现数据访问速度是瓶颈时，可以考虑加缓存。当然，缓存的管理也会很复杂，管理方式的不同也会从各种角度影响到缓存的效果。因此，用与不用，就是另一个故事了。</p><h2 id="小结"><a class="header-anchor" href="#小结">#</a>小结</h2><p>架构嘛，都是 trade-off。这“三件套”更多是方法论角度上的指导。真实使用时如何权衡各种方法的利弊，就需要具体问题具体分析了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;提高性能最怕是没有方向，没有头绪的时候不妨试试三件套：并行、异步、加缓存。&lt;/p&gt;
&lt;h2 id=&quot;并行&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#并行&quot;&gt;#&lt;/a&gt;并行&lt;/h2&gt;
&lt;p&gt;并行比较容易理解，一个人做不完的事，多喊几个人一起做。从算法的
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>决策树 (decision tree)</title>
    <link href="http://lotabout.github.io/2018/decision-tree/"/>
    <id>http://lotabout.github.io/2018/decision-tree/</id>
    <published>2018-03-02T14:40:56.000Z</published>
    <updated>2018-07-29T02:13:50.121Z</updated>
    
    <content type="html"><![CDATA[<p>通过训练，我们可以从样本中学习到决策树，作为预测模型来预测其它样本。两个问题：</p><ol><li>我们说要训练/学习，训练/学习什么？</li><li>为什么决策树可以用来预测？或者说它的泛化能力的来源是哪？</li></ol><a id="more"></a><h2 id="什么是决策树？"><a class="header-anchor" href="#什么是决策树？">#</a>什么是决策树？</h2><p>一棵“树”，目的和作用是“决策”。一般来说，每个节点上都保存了一个切分，输入数据通过切分继续访问子节点，直到叶子节点，就找到了目标，或者说“做出了决策”。这里我们举个喜闻乐见的例子吧。</p><p>现在有人给你介绍对象，你打听到对方的特点：白不白，富不富，美不美，然后决定去不去相亲。根据以往经验，我们给出所有可能性：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">| 白   | 富   | 美   | 去   |</span><br><span class="line">| 白   | 富   | 不美 | 去   |</span><br><span class="line">| 白   | 不富 | 美   | 犹豫 |</span><br><span class="line">| 白   | 不富 | 不美 | 犹豫 |</span><br><span class="line">| 不白 | 富   | 美   | 去   |</span><br><span class="line">| 不白 | 富   | 不美 | 去   |</span><br><span class="line">| 不白 | 不富 | 美   | 犹豫 |</span><br><span class="line">| 不白 | 不富 | 不美 | 不去 |</span><br></pre></td></tr></table></figure><p>那么有人给我们介绍新的对象的时候，我们就要一个个特点去判断，于是这种判断的过程就可以画成一棵树，例如根据特点依次判断：</p><img src="/2018/decision-tree/decision-tree-abc.1.png" title="Decision Tree ABC Full"><p>这就是决策树，每一层我们都提出一个问题，根据问题的回答来走向不同的子树，最终到达叶子节点时，做出决策（去还是不去）。可以看到，决策树没有任何特别的地方。</p><p>当然，如果我们先考虑富不富，再考虑白不白，则得到的树又不相同：</p><img src="/2018/decision-tree/decision-tree-bac.1.png" title="Decision Tree BAC Full"><p>所以，决策树其实就是根据已知的经验来构建一棵树。可以认为是根据数据的某个维度进行切分，不断重复这个过程。当然，如果切分的顺序不同，会得到不同的树。</p><p>既然如此，按不同顺序切分得到的决策树又有什么不同呢？</p><h2 id="训练-训练什么？"><a class="header-anchor" href="#训练-训练什么？">#</a>训练，训练什么？</h2><p>如果仔细观察，我们发现决策树中有一些叶子节点是可以合并的，合并之后，到达某个节点时就不需要进行额外的决策，例如切分顺序“白，富，美”得到的决策树合并后如下：</p><img src="/2018/decision-tree/decision-tree-abc.2.png" title="Decision Tree ABC Abbreviate"><p>我们先记着，合并后的树有 5 个叶子节点。</p><p>而“富，白，美”的决策树合并后变成：</p><img src="/2018/decision-tree/decision-tree-bac.2.png" title="Decision Tree BAC Abbreviate"><p>可以看到上面这棵树则只有 4 个叶子节点，少于“白，富，美”的 5 个节点。</p><p>这就是决策树间最大的区别，不同决策树合并后得到树叶子节点的个数是不同的，后面我们会看到，叶子节点越少，往往决策树的泛化能力越高，所以可以认为训练决策树的一个目标是 <strong>减少决策树的叶子节点</strong> 。这个任务其实是很困难的，考虑数据有 <code>n</code> 维，那么切分的顺序的可能性就是<code>n!</code>。因此实际中一般并不是求全局最优，而是采用贪心算法求局部最优。</p><p>另外，节点在什么时候才能合并呢？一般需要叶子节点的标签/决策相同。也因此，后面提到贪心的指标时，往往指标的目的就是选择某一个维度，使得划分后的子集合更 <strong>有序</strong>。</p><p>（当然上面的说法不准确，决策树就是一棵树，建成什么样子其实全凭心情/需求。在搜索过程中其实并找不到官方的优化目标，上面的结论是博主自己得出的，而它能帮助我们理解下一个问题：决策树为什么有泛化能力？）</p><h2 id="泛化能力"><a class="header-anchor" href="#泛化能力">#</a>泛化能力</h2><p>细心的读者会发现，决策树好像根本没什么用？</p><p>在上面的例子里，我们只需要记住切分的顺序，例如“富，白美”，然后在原数据中一个个匹配就行了，树的结构虽然方便理解，但它也没有存在的必要。而这个疑问的一个引申，既然我们能通过查表来做决策，但之前又说决策树可以用来做预测，那么决策树的泛化能力（即“预测”能力）来自哪里呢？</p><h3 id="节点的合并是泛化能力的根本"><a class="header-anchor" href="#节点的合并是泛化能力的根本">#</a>节点的合并是泛化能力的根本</h3><p>上面的例子中我们有三个维度，每个维度有两种可能，并且我们的经验已经覆盖了所有的8 种情况。但实际生活中我们的样本不可能覆盖所有的可能性，因此在我们合并节点的过程中就悄悄地覆盖了一些未知的数据。例如我们只遇到过 4 种情况（这里的决策和上面的例子不同）：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">| 白   | 富   | 美   | 去   |</span><br><span class="line">| 白   | 不富 | 美   | 犹豫 |</span><br><span class="line">| 不白 | 不富 | 美   | 犹豫 |</span><br><span class="line">| 不白 | 不富 | 不美 | 不去 |</span><br></pre></td></tr></table></figure><p>在此基础上构建决策树（顺序为富，白，美）：</p><img src="/2018/decision-tree/decision-tree-missing.1.png" title="Decision Tree Missing Full"><p>于是在合并的过程中，我们把没有见过数据都忽略，于是合并后的树为：</p><img src="/2018/decision-tree/decision-tree-missing.2.png" title="Decision Tree Missing Abbreviate"><p>对于这棵决策树，如果遇到新的数据 <code>白，富，不美</code> 我们也可以推测值得 <code>去</code>，或者对于 <code>白，不富，不美</code> 我们就会 <code>犹豫</code>。</p><p>当然，我们的示例自下向上合并，是为了方便展示，实际构建决策树时，如果某个分支只有一个样本，会直接停止展开。如，在“富”的分支上，我们只见到一个样本，结果是“去”，因此会停止继续向下展开。这和算法中的剪枝是相同的想法。</p><h3 id="类别的切分也能提供泛化能力"><a class="header-anchor" href="#类别的切分也能提供泛化能力">#</a>类别的切分也能提供泛化能力</h3><p>上面的例子里我们的数据都是“类别型”，即一个维度/特征的取值是离散的，例如“富”，只能取两个值“富”和“不富”。但实际生活中，一个维度的取值可以是连续的，例如人的身高，体重，工资等。</p><p>那么，当决策树的一个节点需要切分时，我们不可能穷举所有的可能，因此需要做一定的取舍。常见的作法是对数据做一个“二切分”。例如我们知道三个人的身高： <code>153</code>,<code>164</code>, <code>182</code>，我们切分成 <code>&lt;= 164</code> 和 <code>&gt; 164</code> 两类（也可以用其它的切分方法）。</p><p>这种 <strong>模糊的切分</strong> 也提供了泛化能力。例如一个新的数据，身高 <code>175</code>，我们自然就能归到 <code>&gt; 164</code> 的切分中，即使之前根本没见过这个数据。</p><h2 id="贪心指标"><a class="header-anchor" href="#贪心指标">#</a>贪心指标</h2><p>一般我们是用贪心算法来构建决策树，这就引申出了一些常用的指标，帮助我们决定在每次切分时，选择哪个维度进行切分；遇到数值类型需要做二切分时，具体用哪个数值。下面我们介绍两个常用的指标：</p><h3 id="基尼不纯度-gini-impurity"><a class="header-anchor" href="#基尼不纯度-gini-impurity">#</a>基尼不纯度 Gini impurity</h3><p>一个集合有 $J$ 个类别，我们记 $i \in { 1, 2, …, J }$，且 $p_i$ 表示该集合中标记为类别 $i$ 的元素所占的比例，则 <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" target="_blank" rel="noopener">基尼不纯度</a> 定义为：</p><p>$$I_{G}( p ) = \sum_{i=1}^J p_i \sum_{k\neq i} p_k= \sum_{i=1}^{J} p_i (1-p_i)= \sum_{i=1}^{J} (p_i - {p_i}^2)= \sum_{i=1}^J p_i - \sum_{i=1}^{J} {p_i}^2= 1 - \sum^{J}_{i=1} {p_i}^{2}$$</p><p>想象我们有一堆乒乓球，和一堆标签，为每个球上贴一个标签，这组成了我们的原始样本。现在，我们再买和之前一样的一堆标签，为每个球上再贴一个标签。那么现在球上有两个标签，它们可能一样，也可能不一样。基尼不纯度指的就是贴了不同标签的球的占比。一个很直观的结论是，如果集合里的标签都一样，那么基尼不纯度就为 <code>0</code>。</p><p>所以在数学上，我们可以先考虑标签 $i$，一个球上第一个标签贴为 $j$ 的概率记为$p_i$，那么贴第二个标签时，要求贴的是非 $i$ 的标签，因此概率是 $\sum_{k \nei}{p_k} = 1 - p_i$。那么贴了不同标签的球所占的比例就是 $\sum_{i=1}^J p_i\sum_{k\neq i} p_k$。</p><p>上面提到，决定用哪个维度进行切分时，一个标准是使切分后的子集更 <strong>有序</strong>，这里也意味着基尼不纯度更小。于是我们选择某一个维度进行切分，求得所有子集的基尼不纯度之和。总有一个维度使得这个和取到最小，对应的维度就是当前最佳的切分维度。当然，维度确定后，对于数值型的维度，其实还要确认具体的切分点，也可以用基尼不纯度来作为切分的依据。</p><h3 id="信息熵增益-information-gain"><a class="header-anchor" href="#信息熵增益-information-gain">#</a>信息熵增益 Information Gain</h3><p>首先要了解的是 <a href="https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)" target="_blank" rel="noopener">信息熵</a> 在有限样本时定义为：</p><p>$$H(X) = \sum_{i}{P(x_i)I(x_i)} = -\sum_i{P(x_i)\log_2{P(x_i)}}$$</p><p>$-\log_2{P(x_i)}$ 的大意是一个事件 $x_i$ 如果出现的概率越小，那么当它发生时我们就越吃惊，代表的就是一个事件“吃惊程度”。而熵就是所有事件的“吃惊程度”的期望值。一般地，如果一个集合的熵越大，则集合越无序；熵越小，则集合越有序。换句话说，如果熵越大，说明我们越容易吃惊，说明集合无序，我们很难预测下一个出现的是什么，相反，熵越小，说明我们越容易猜测集合里有什么，说明集合越有序。</p><p>而在决策树的切分里，事件 $x_i$ 可以认为是在样本中出现某个标签/决策。于是$P(x_i)$可以用所有样本中某个标签出现的频率来代替。</p><p>但我们求熵是为了决定采用哪一个维度进行切分，因此有一个新的概念 <a href="https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E7%86%B5" target="_blank" rel="noopener">条件熵</a>：</p><p>$$ H(X|Y) = \sum_{y \in Y}{p(y) H(X|Y=y)} $$</p><p>这里我们认为 $Y$ 就是用某个维度进行切分，那么 $y$ 就是切成的某个子集合于是$H(X|Y=y)$ 就是这个子集的熵。因此可以认为就条件熵是每个子集合的熵的一个加权平均/期望。最后，如何判断一个维度更优秀呢？我们采用信息熵增益：</p><p>$$Gain(Y) = H(X) - H(X|Y)$$</p><p>即切分后，<code>Gain</code> 最高的那个维度，我们优先用它来切分子集。</p><h2 id="决策树的实现"><a class="header-anchor" href="#决策树的实现">#</a>决策树的实现</h2><p>这里用 python 来实现一下基本的决策树（非数值型），再用上面的例子实验实验。完整代码请见<a href="https://gist.github.com/lotabout/ae2401b091bd7faf4ae6230666f53568/2844bb083d976a21a56f4acf0080b2be35ee28b9" target="_blank" rel="noopener">gist</a>。</p><p>首先决定输入的结构：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [[<span class="string">'白'</span>,   <span class="string">'富'</span>,   <span class="string">'美'</span>,   <span class="string">'去'</span>],</span><br><span class="line">        [<span class="string">'白'</span>,   <span class="string">'富'</span>,   <span class="string">'不美'</span>, <span class="string">'去'</span>],</span><br><span class="line">        [<span class="string">'白'</span>,   <span class="string">'不富'</span>, <span class="string">'美'</span>,   <span class="string">'犹豫'</span>],</span><br><span class="line">        [<span class="string">'白'</span>,   <span class="string">'不富'</span>, <span class="string">'不美'</span>, <span class="string">'犹豫'</span>],</span><br><span class="line">        [<span class="string">'不白'</span>, <span class="string">'富'</span>,   <span class="string">'美'</span>,   <span class="string">'去'</span>],</span><br><span class="line">        [<span class="string">'不白'</span>, <span class="string">'富'</span>,   <span class="string">'不美'</span>, <span class="string">'去'</span>],</span><br><span class="line">        [<span class="string">'不白'</span>, <span class="string">'不富'</span>, <span class="string">'美'</span>,   <span class="string">'犹豫'</span>],</span><br><span class="line">        [<span class="string">'不白'</span>, <span class="string">'不富'</span>, <span class="string">'不美'</span>, <span class="string">'不去'</span>]]</span><br></pre></td></tr></table></figure><p>数据是一个 List，每一个元素也是一个 List，代表样本的多个维度，最后一维存放标签。</p><p>下面先实现一个切分的函数，作用是将一系列样本，根据某个维度，切分到不同的集合。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_split_samples</span><span class="params">(self, samples, feature)</span>:</span></span><br><span class="line">    <span class="string">"""Split samples into subsets, according to the feature</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :samples: List[List[val]]</span></span><br><span class="line"><span class="string">    :feature: Int</span></span><br><span class="line"><span class="string">    :returns: &#123;val: List[data]&#125; a dict contains the data of subsets</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ret = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> sample <span class="keyword">in</span> samples:</span><br><span class="line">        val = sample[feature]</span><br><span class="line">        ret.setdefault(val, [])</span><br><span class="line">        ret[val].append(sample)</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure><p>有了切分的函数我们就能创建决策树了，下面这个函数是递归调用，给定一些数据，如果<code>_stop_now</code> 判断已经不需要继续切分了，则返回这些数据的标签（一般来说这些数据的标签会相同），否则我们调用 <code>_get_feature</code> 来决定用哪个维度进行切分，并对每个子集合调用递归调用 <code>_split</code> 创建节点。</p><p>树的节点我们用 dict 表示，例如 <code>{'白': ..., '不白', ...}</code>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_split</span><span class="params">(self, data, level=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""recursively split the data for node</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :data: List[data]</span></span><br><span class="line"><span class="string">    :returns: label if should stop, else a node of the tree</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._stop_now(data):</span><br><span class="line">        <span class="keyword">return</span> data[<span class="number">0</span>][<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># split the data</span></span><br><span class="line">    feature = self._get_feature(data, level)</span><br><span class="line">    subsets = self._split_samples(data, feature)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;key: self._split(subset, level+<span class="number">1</span>) <span class="keyword">for</span> key, subset <span class="keyword">in</span> subsets.items()&#125;</span><br></pre></td></tr></table></figure><p>接下来，我们只需要实现 <code>_stop_now</code> 和 <code>_get_feature</code> 就可以了。对于<code>_stop_now</code>，我们认为如果所有样本都是同一个标签就可以停止：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_stop_now</span><span class="params">(self, data)</span>:</span></span><br><span class="line">    <span class="string">"""check if we need to stop now</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :data: List[data]</span></span><br><span class="line"><span class="string">    :returns: Boolean</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    labels = [d[<span class="number">-1</span>] <span class="keyword">for</span> d <span class="keyword">in</span> data]</span><br><span class="line">    <span class="keyword">return</span> len(set(labels)) &lt;= <span class="number">1</span></span><br></pre></td></tr></table></figure><p>而 <code>_get_feature</code>，我们按输入的维度顺序切分，因此实现是：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_feature</span><span class="params">(self, data, level)</span>:</span></span><br><span class="line">    <span class="string">"""Decide which feature to be used to split data</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :data: List[data]</span></span><br><span class="line"><span class="string">    :level: Int the level of the tree</span></span><br><span class="line"><span class="string">    :returns: Int the dimension of the data to be used for split data</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> level</span><br></pre></td></tr></table></figure><p>最后把上面这些代码放到一个类里：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTree</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="string">"""Learn a decision tree from data and label</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :data: List[List[val]], a list contains M sample, each sample is represented by a List</span></span><br><span class="line"><span class="string">               The last column of the sample is the label</span></span><br><span class="line"><span class="string">        :returns: The root of a decision tree</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        super(DecisionTree, self).__init__()</span><br><span class="line">        self.root = self._split(data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># rest of the methods</span></span><br><span class="line"></span><br><span class="line">tree = DecisionTree(data)</span><br><span class="line">print(tree.root)</span><br></pre></td></tr></table></figure><p>得到的结果是：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    '白': &#123;</span><br><span class="line">        '富': '去',</span><br><span class="line">        '不富': '犹豫'</span><br><span class="line">    &#125;,</span><br><span class="line">    '不白': &#123;</span><br><span class="line">        '富': '去',</span><br><span class="line">        '不富': &#123;</span><br><span class="line">            '美': '犹豫',</span><br><span class="line">            '不美': '不去'</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，和我们前面手工合并的结果是一样的。</p><h3 id="熵增益"><a class="header-anchor" href="#熵增益">#</a>熵增益</h3><p>下面我们实现信息熵增益指标，首先是熵的计算</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_entropy</span><span class="params">(self, dataset)</span>:</span></span><br><span class="line">    <span class="string">"""calculate the entropy of a dataset</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :dataset: List[data], each data is List[val], last column is label</span></span><br><span class="line"><span class="string">    :returns: Float</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    counts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataset:</span><br><span class="line">        label = data[<span class="number">-1</span>]</span><br><span class="line">        counts.setdefault(label, <span class="number">0</span>)</span><br><span class="line">        counts[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    total_num = len(dataset)</span><br><span class="line">    <span class="keyword">return</span> sum([-count/total_num * math.log2(count/total_num) <span class="keyword">for</span> count <span class="keyword">in</span> counts.values()])</span><br></pre></td></tr></table></figure><p>然后是条件熵的计算：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_conditional_entropy</span><span class="params">(self, dataset, feature)</span>:</span></span><br><span class="line">    <span class="string">"""calculate the conditional entropy of dataset on feature</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :dataset: List[data]</span></span><br><span class="line"><span class="string">    :feature: Int</span></span><br><span class="line"><span class="string">    :returns: Float</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    subsets = self._split_samples(dataset, feature)</span><br><span class="line">    total_num = len(subsets)</span><br><span class="line">    <span class="keyword">return</span> sum([len(subset)/total_num * self._entropy(subset) <span class="keyword">for</span> subset <span class="keyword">in</span> subsets.values()])</span><br></pre></td></tr></table></figure><p>最后，替换之前的 <code>_get_feature</code>，也就是在决定用什么维度进行切分时，我们选择熵增益最大的维度：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_feature</span><span class="params">(self, data, level)</span>:</span></span><br><span class="line">    dimensions = len(data[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    entropy = self._entropy(data)</span><br><span class="line"></span><br><span class="line">    gains = [entropy - self._conditional_entropy(data, i) <span class="keyword">for</span> i <span class="keyword">in</span> range(dimensions)]</span><br><span class="line">    <span class="keyword">return</span> gains.index(max(gains))</span><br></pre></td></tr></table></figure><p>我们再用这个策略去“训练”前面的数据，得到的结果为：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    '富': '去',</span><br><span class="line">    '不富': &#123;</span><br><span class="line">        '白': '犹豫',</span><br><span class="line">        '不白': &#123;</span><br><span class="line">            '美': '犹豫',</span><br><span class="line">            '不美': '不去'</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，结果对应了“富，白，美”的切分顺序。而之前我们也知道，这个顺序有 4 个叶子节点，而默认切分 “白，富，美” 有 5 个叶子节点。也证明这样的优化目标是有效的。</p><h2 id="小结"><a class="header-anchor" href="#小结">#</a>小结</h2><p>“训练”决策树是为了减少决策树最后的叶子节点，由于训练全局最优很困难，因此人们用一些局部的贪心策略进行训练，例如上文介绍的信息熵增益。</p><p>决策树的泛化能力主要来源于叶节点的合并。因此，如果决策树“过拟合”，其实意味着合并的节点不够多。</p><p>最后，本文代码的完整版请见 <a href="https://gist.github.com/lotabout/ae2401b091bd7faf4ae6230666f53568" target="_blank" rel="noopener">Gist: decision tree</a></p><h2 id="参考"><a class="header-anchor" href="#参考">#</a>参考</h2><ul><li><a href="http://www.csuldw.com/2015/05/08/2015-05-08-decision%20tree/" target="_blank" rel="noopener">http://www.csuldw.com/2015/05/08/2015-05-08-decision tree/</a> 对优化指标有很好的讲解。</li><li><a href="http://blog.csdn.net/xbinworld/article/details/44660339" target="_blank" rel="noopener">http://blog.csdn.net/xbinworld/article/details/44660339</a> 讲解了一些实现上的注意点，如过拟合，剪枝。</li><li><a href="https://www.geeksforgeeks.org/decision-tree-introduction-example/" target="_blank" rel="noopener">https://www.geeksforgeeks.org/decision-tree-introduction-example/</a> 包含了数值型数据的一些实现</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过训练，我们可以从样本中学习到决策树，作为预测模型来预测其它样本。两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们说要训练/学习，训练/学习什么？&lt;/li&gt;
&lt;li&gt;为什么决策树可以用来预测？或者说它的泛化能力的来源是哪？&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Knowledge" scheme="http://lotabout.github.io/categories/Knowledge/"/>
    
    
      <category term="decision tree" scheme="http://lotabout.github.io/tags/decision-tree/"/>
    
      <category term="Statistics" scheme="http://lotabout.github.io/tags/Statistics/"/>
    
  </entry>
  
  <entry>
    <title>核密度估计（kernel density estimation）</title>
    <link href="http://lotabout.github.io/2018/kernel-density-estimation/"/>
    <id>http://lotabout.github.io/2018/kernel-density-estimation/</id>
    <published>2018-02-27T14:43:05.000Z</published>
    <updated>2018-07-29T02:13:50.125Z</updated>
    
    <content type="html"><![CDATA[<p>有一些数据，想“看看”它长什么样，我们一般会画直方图（Histogram）。现在你也可以用核密度估计。</p><h2 id="什么是-核"><a class="header-anchor" href="#什么是-核">#</a>什么是“核”</h2><p>如果不了解背景，看到“核密度估计”这个概念基本上就是一脸懵逼。我们先说说这个核(<a href="https://en.wikipedia.org/wiki/Kernel_(statistics)" target="_blank" rel="noopener">kernel</a>) 是什么。</p><p>首先，“核”在不同的语境下的含义是不同的，例如在模式识别里，它的含义就和这里不同。在“非参数估计”的语境下，“核”是一个函数，用来提供权重。例如高斯函数(Gaussian) 就是一个常用的核函数。</p><p>让我们举个例子，假设我们现在想买房，钱不够要找亲戚朋友借，我们用一个数组来表示5 个亲戚的财产状况： <code>[8, 2, 5, 6, 4]</code>。我们是中间这个数 <code>5</code>。“核”可以类比 成朋友圈，但不同的亲戚朋友亲疏有别，在借钱的时候，关系好的朋友出力多，关系不好的朋友出力少，于是我们可以用权重来表示。总共能借到的钱是： <code>8*0.1 + 2*0.4 + 5 + 6*0.3 + 4*0.2 = 9.2</code>。</p><p>那么“核”的作用就是用来决定权重，例如高斯函数（即正态分布）：</p><p><img src="https://upload.wikimedia.org/wikipedia/commons/0/0f/Kernel_exponential.svg" alt="Kernel Exponential"></p><p>如果还套用上面的例子的话，可以认为在 3 代血亲之外的亲戚就基本不会借钱给你了。</p><p>最后呢，一般要求核函数有下面两个性质：</p><ul><li>归一化：$\int_{- \infty}^{+ \infty}{K(u) du} = 1$</li><li>对称性：对所有 $u$ 要求 $K(-u) = K(u)$</li></ul><p>最后的最后： <a href="https://en.wikipedia.org/wiki/Kernel_(statistics)#Kernel_functions_in_common_use" target="_blank" rel="noopener">一些常用的核</a></p><h2 id="核密度估计"><a class="header-anchor" href="#核密度估计">#</a>核密度估计</h2><p>理解了“核”，核密度估计就容易理解了。</p><p>如果我们画直方图，其实目的是画出“概率密度函数”，而直方图本质上是认为频率等于概率。但这种假设不是必然的。核密度函数就是一种“平滑(smooth)”的手段。相当于是“我说我很牛逼你可能不信，但你可以听听我的朋友们是怎么评价我的，加权平均下就能更好地了解我了”。于是乎：</p><p>设 $(x_1, x_2, …, x_n)$ 是独立同分布的 n 个样本点，它的概率密度函数是 $f$，于是我们的估计：</p><p>$$\hat{f_h}(x) = \frac{1}{n}\sum_{i=1}^{n}{K_h(x - x_i)} = \frac{1}{nh}\sum_{i=1}^{n}{K(\frac{x-x_i}{h})}$$</p><p>上面式子中 <code>h</code> 是人为指定的，代表“朋友圈”的大小，正式的叫法是“带宽”(bandwidth)。而 $x-x_i$ 就是自己与朋友的亲疏程度，当然最后要正归化到 <code>[-1, 1]</code> 之间。下图是直方图和核密度估计的一个对比：</p><p><img src="https://upload.wikimedia.org/wikipedia/commons/4/41/Comparison_of_1D_histogram_and_KDE.png" alt="Comparison_of_1D_histogram_and_KDE"></p><h2 id="选择合适的带宽"><a class="header-anchor" href="#选择合适的带宽">#</a>选择合适的带宽</h2><p>选择不同的带宽，核密度估计的结果也大不相同，因此人们研究了一些算法来选择带宽。这方面对理解 KDE 本身没有什么太重要的意义，并且常见的算法在 scipy 里也已经都实现了，这里就不细说了，有兴趣的看看 <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation#Bandwidth_selection" target="_blank" rel="noopener">wiki</a> 吧。</p><h2 id="参考"><a class="header-anchor" href="#参考">#</a>参考</h2><ul><li><a href="%E9%9D%9E%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%EF%BC%9A%E6%A0%B8%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1KDE">http://blog.csdn.net/pipisorry/article/details/53635895</a></li><li><a href="http://blog.shaochuancs.com/statistics-kde/" target="_blank" rel="noopener">一维数据可视化：核密度估计(Kernel Density Estimates)</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;有一些数据，想“看看”它长什么样，我们一般会画直方图（Histogram）。现在你也可以用核密度估计。&lt;/p&gt;
&lt;h2 id=&quot;什么是-核&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#什么是-核&quot;&gt;#&lt;/a&gt;什么是“核”&lt;/h2&gt;
&lt;p&gt;如果不了解背景
      
    
    </summary>
    
      <category term="Knowledge" scheme="http://lotabout.github.io/categories/Knowledge/"/>
    
    
      <category term="Statistics" scheme="http://lotabout.github.io/tags/Statistics/"/>
    
      <category term="KDE" scheme="http://lotabout.github.io/tags/KDE/"/>
    
  </entry>
  
</feed>
